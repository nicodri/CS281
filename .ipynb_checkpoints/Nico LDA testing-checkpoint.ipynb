{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The goal is here to compare the different lda that we coded so far. It's time to pick the best one, to test our data we already used a synthetic dataset which works alost perfectly with all these models.\n",
    "Now we can try on the Edimbourgh dataset (we don't really have a way to evaluate its performance actually) and on the AP corpus (here we can compare with Blei results and we know that the data are clean enough).\n",
    "\n",
    "On the edimbourgh, we have a lot of redundant words in the different topics. I think it's because of the nature of the data because all the models provide these results, it may not be only an algo issue.\n",
    "We should gather more data from the Yelp review to experiment.\n",
    "\n",
    "On the AP corpus, the lda package provides really good results for 100 iterations in around 15 minutes whereas our models (only got result for the lda minibatch of size 10 without ordering and lda batch with executed in 2h30) provide poor results with still a lot of repetitions.\n",
    "It seems that we are missing something because Blei and the gibbs sampler version provides better results.\n",
    "\n",
    "Based on our experiments, we should focus on the mini-batch lda (for a decent number of docs per batch, to prevent the sensitivity at the initialization) and benchmarking the necessity to order also.\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "* Improving the mini-batch lda:\n",
    "    * Functional Cross validation for topics number and (tau, kappa)\n",
    "    * Compute the gamma matrix on a larger review dataset from Yelp (for instance: Las Vegas)\n",
    "    \n",
    "* Multi-class classification: Using the features from the documents-topics distribution in the models\n",
    "    * multi class logistic regression\n",
    "    * one-vs-all SVM\n",
    "    \n",
    "* Recommender system: I listed the different possibilities\n",
    "    * Latent features: from a matrix factorisation on the sparse matrix of ratings (user_id, business) to find the user and restaurants features.\n",
    "    * Computation of a similarity graph between the restaurant: use of the euclidian distance on the different businesses with their features (or in two stages: KL divergence on the topic distribution as it's a probability distribution for each document ==> provide one KL score for each business then compute the euclidean distance on the other numerical feature (checkin, categories...) concatenate to the KL score (possibility of adding a weight to emphasize the KL divergence))\n",
    "    * Clustering algorithms on our numerical features (k-means)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import LDA\n",
    "import lda\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import psi\n",
    "import collections\n",
    "import json\n",
    "from scipy import sparse\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('temp/Edi10.json', 'r') as fp:\n",
    "    reduced_edi = json.load(fp)\n",
    "\n",
    "with open('temp/word_to_index.json', 'r') as fp:\n",
    "    word_to_index = json.load(fp)\n",
    "\n",
    "with open('temp/index_to_word.json', 'r') as fp:\n",
    "    index_to_word = json.load(fp)\n",
    "\n",
    "with open('temp/bid_to_index.json', 'r') as fp:\n",
    "    bid_to_index = json.load(fp)\n",
    "\n",
    "with open('temp/Edi10.json', 'r') as fp:\n",
    "    index_to_bid = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab10 = word_to_index.keys()\n",
    "nonzero_data = []\n",
    "rows_s = []\n",
    "cols_s = []\n",
    "\n",
    "for k in reduced_edi.keys():\n",
    "    counter = collections.Counter(reduced_edi[k])\n",
    "    nonzero_data += counter.values()\n",
    "    rows_s += [bid_to_index[k]]*len(counter.values())\n",
    "    cols_s += [word_to_index[ck] for ck in counter.keys()]\n",
    "\n",
    "sparse_mat = sparse.csc_matrix((nonzero_data,(rows_s,cols_s)),shape = (len(bid_to_index),len(word_to_index)))\n",
    "\n",
    "dtm_edi = sparse_mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(667, 2313)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_edi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Routines used in the lda\n",
    "def e_step(d, corpus, gamma_d_k, lambda_, lambda_int, alpha, threshold):\n",
    "    '''\n",
    "    Routine for the lda function\n",
    "    '''\n",
    "    # Info for the current doc\n",
    "    ids = np.nonzero(corpus[d, :])[0]\n",
    "    counts = corpus[d, ids]\n",
    "\n",
    "    gamma_d = gamma_d_k[d, :]\n",
    "    E_log_beta = digamma(lambda_)[:, ids]\n",
    "    for t in xrange(100):  # TODO: Wait for convergence\n",
    "        # Used to check convergence\n",
    "        old_gamma = gamma_d\n",
    "\n",
    "        E_log_thetad = digamma(gamma_d)\n",
    "\n",
    "        # shape of phi is (num_topics, len(ids))\n",
    "        phi = np.exp(E_log_beta + E_log_thetad[:, np.newaxis])\n",
    "        phi /= phi.sum(axis=0)\n",
    "\n",
    "        gamma_d = alpha + np.dot(phi, counts)\n",
    "\n",
    "\n",
    "        # Check if convergence\n",
    "        if (np.mean((gamma_d - old_gamma)**2) < threshold):\n",
    "            break\n",
    "\n",
    "    gamma_d_k[d, :] = gamma_d\n",
    "    lambda_int[:, ids] += counts[np.newaxis, :] * phi\n",
    "\n",
    "    return gamma_d_k, lambda_int\n",
    "\n",
    "def digamma(data):\n",
    "    if (len(data.shape) == 1):\n",
    "        return psi(data) - psi(np.sum(data))\n",
    "    return psi(data) - psi(np.sum(data, axis=1))[:, np.newaxis]\n",
    "\n",
    "\n",
    "def get_samples(C, S, max_iter):\n",
    "    batches_temp = np.zeros(C * max_iter, dtype=int)\n",
    "    sample = np.arange(C, dtype=int)\n",
    "    for k in xrange(max_iter):\n",
    "        batches_temp[k * C: (k + 1) * C] = sample\n",
    "    # List of mini-batches\n",
    "    batches = np.split(batches_temp, C * max_iter / S)\n",
    "    return batches\n",
    "\n",
    "# Display the selected topics\n",
    "def print_topic_words(lambda_, vocabulary, num_topics, num_words):\n",
    "    '''\n",
    "    Display the first num_words for the topic distribution lambda_ from a\n",
    "    vocabulary.\n",
    "    '''\n",
    "    for t in xrange(num_topics):\n",
    "        topic_distribution = sorted([(i, p) for i, p in enumerate(lambda_[t, :])], key=lambda x: x[1], reverse=True)\n",
    "        top_words = [vocabulary[tup[0]] for tup in topic_distribution[:num_words]]\n",
    "        print 'Topic number ', t\n",
    "        print top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_lda(corpus, lambda_=None, num_topics=10, num_iter=10, alpha=0.5, eta=0.001, threshold=0.000001):\n",
    "    '''\n",
    "    Batch Variational Inference EM algorithm for LDA.\n",
    "    (from algorithm 1 in Blei 2010)\n",
    "    corpus is a list of lists of [word_index, count] for each document\n",
    "    corpus is a matrix of count: (docs, voca)\n",
    "    Args:\n",
    "        lambda_: to set a specific lambda for the initialization\n",
    "    '''\n",
    "    C, V = corpus.shape\n",
    "\n",
    "    # Initialisation\n",
    "    if not np.any(lambda_):\n",
    "        lambda_ = np.random.gamma(100, 1./100, size=(num_topics, V))\n",
    "    else:\n",
    "        lambda_ = lambda_.copy()\n",
    "\n",
    "    gamma_d_k = np.ones((C, num_topics))\n",
    "    sample = range(C)\n",
    "    np.random.shuffle(sample)\n",
    "\n",
    "    for t in xrange(num_iter):\n",
    "        old_lambda_ = lambda_\n",
    "        # #### E-step\n",
    "        lambda_int = np.zeros((num_topics, V))\n",
    "        for d in sample:\n",
    "            gamma_d_k, lambda_int = e_step(d, corpus, gamma_d_k, lambda_,\n",
    "                                           lambda_int, alpha, threshold)\n",
    "\n",
    "        # #### M-step\n",
    "        lambda_ = eta + lambda_int\n",
    "\n",
    "        # Check if convergence\n",
    "        if (np.mean(np.abs((lambda_ - old_lambda_) / old_lambda_)) < threshold):\n",
    "            break\n",
    "\n",
    "    return lambda_, gamma_d_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_lda_ordering(corpus, lambda_=None, S=1, num_topics=10, max_iter=300, tau=1, kappa=0.5, alpha=0.5, eta=0.001, threshold=0.000001):\n",
    "    '''\n",
    "    Stochastic Variational Inference EM algorithm for LDA.\n",
    "    (from algorithm 2 in Blei 2010)\n",
    "    corpus is a list of lists of [word_index, count] for each document\n",
    "    corpus is a matrix of count: (docs, voca)\n",
    "    Args:\n",
    "        lambda_: to set a specific lambda for the initialization\n",
    "        batches: to set an order on the use of the corpus\n",
    "        S: size of the mini-batches\n",
    "    '''\n",
    "    C, V = corpus.shape\n",
    "\n",
    "    # Initialisation\n",
    "    if not np.any(lambda_):\n",
    "        lambda_ = np.random.gamma(100, 1./100, size=(num_topics, V))\n",
    "    else:\n",
    "        lambda_ = lambda_.copy()\n",
    "\n",
    "    gamma_d_k = np.ones((C, num_topics))\n",
    "\n",
    "    # Sampling\n",
    "    idx = range(C)\n",
    "    # TODO: better splitting because here we may not consider some documents\n",
    "    # and could raise an error if C/S does not split idx in equal parts.\n",
    "    batches = np.array_split(idx, C/S)\n",
    "    \n",
    "    for it in xrange(max_iter):\n",
    "        for t in xrange(len(batches)):\n",
    "            # #### E-step\n",
    "            lambda_int = np.zeros((num_topics, V))\n",
    "\n",
    "            for d in batches[t]:\n",
    "                gamma_d_k, lambda_int = e_step(d, corpus, gamma_d_k, lambda_, lambda_int, alpha, threshold)\n",
    "\n",
    "            # #### M-step\n",
    "            rho = (tau + t)**(-kappa)\n",
    "            indices = np.unique(np.nonzero(corpus[batches[t], :])[1])\n",
    "            lambda_int = eta + C / (1. * S) * lambda_int\n",
    "            lambda_[:, indices] = (1 - rho)*lambda_[:, indices] + rho*lambda_int[:, indices]\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            sorted_model  = sklearn.cluster.KMeans(n_clusters  = num_topics)\n",
    "            sorted_index = np.argsort(sorted_model.fit_predict(gamma_d_k))\n",
    "            batches = np.array_split(sorted_index,C/S)\n",
    "\n",
    "    return lambda_, gamma_d_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def stochastic_lda(corpus, batches=None, lambda_=None,\n",
    "                   ordering=False, S=1, num_topics=10, max_iter=300, tau=1,\n",
    "                   kappa=0.5, alpha=0.5, eta=0.001, threshold=0.000001):\n",
    "    '''\n",
    "    Stochastic Variational Inference EM algorithm for LDA.\n",
    "    (from algorithm 2 in Blei 2010)\n",
    "    corpus is a list of lists of [word_index, count] for each document\n",
    "    corpus is a matrix of count: (docs, voca)\n",
    "    Args:\n",
    "        lambda_: to set a specific lambda for the initialization\n",
    "        batches: to set an order on the use of the corpus\n",
    "        S: size of the mini-batches\n",
    "    '''\n",
    "    C, V = corpus.shape\n",
    "\n",
    "    # Initialisation\n",
    "    if not np.any(lambda_):\n",
    "        lambda_ = np.random.gamma(100, 1./100, size=(num_topics, V))\n",
    "    else:\n",
    "        lambda_ = lambda_.copy()\n",
    "\n",
    "    gamma_d_k = np.ones((C, num_topics))\n",
    "\n",
    "    # Sampling\n",
    "    if not np.any(batches):\n",
    "        batches = get_samples(C, S, max_iter)\n",
    "\n",
    "    for t in xrange(len(batches)):\n",
    "        # #### E-step\n",
    "        lambda_int = np.zeros((num_topics, V))\n",
    "\n",
    "        for d in batches[t]:\n",
    "            gamma_d_k, lambda_int = e_step(d, corpus, gamma_d_k, lambda_, lambda_int, alpha, threshold)\n",
    "\n",
    "        # #### M-step\n",
    "        rho = (tau + t)**(-kappa)\n",
    "        indices = np.unique(np.nonzero(corpus[batches[t], :])[1])\n",
    "        lambda_int = eta + C / (1. * S) * lambda_int\n",
    "        lambda_[:, indices] = (1 - rho)*lambda_[:, indices] + rho*lambda_int[:, indices]\n",
    "\n",
    "    return lambda_, gamma_d_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edimbourgh Reviews\n",
    "Test on the edimbourgh reviews with three different models:\n",
    "* batch lda: classical EM version where we iterate over the whole document at each E step (algo 1 in Blei 2010). The update in the M step consider only the new evaluation of lambda in the previous E step (because no approximation is done)\n",
    "* mini-batch lda: we go only over a mini-batch in the E step (here 10). There is the issue relative to the sensitivity to the order of the corpus, this may be handled with the ordering version below.\n",
    "* mini-batch with ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 59s, sys: 783 ms, total: 1min 59s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lambda_batch, gamma_batch = batch_lda(dtm_edi, num_topics=10, num_iter=200, alpha=0.001, eta=0.001, threshold=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 1.22 s, total: 2min 32s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lambda_stochastic, gamma_stochastic = stochastic_lda_ordering(dtm_edi, S=10, num_topics=10, max_iter=200, tau=500, kappa=0.6, alpha=0.001, eta=0.001, threshold=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 22s, sys: 953 ms, total: 2min 23s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lambda_stochastic_no_ordering, gamma_stochastic_no_ordering = stochastic_lda(dtm_edi, S=10, num_topics=10, max_iter=200, tau=500, kappa=0.6, alpha=0.001, eta=0.001, threshold=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 287 ms, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x106781ea8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lda.LDA(10)\n",
    "%time model.fit(dtm_edi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: spanish shop oil mile film measure snag strawberry offer £8\n",
      "Topic 1: component lassi craft energy elephant gnocchi bum beet £5 rosemary\n",
      "Topic 2: fancy website habit elephant brewery satay kudo spanish crepe high\n",
      "Topic 3: spanish oil mile scale employee plate rump gym £8 gathering\n",
      "Topic 4: band problem hash topping charcuterie meaning katsu acoustic challenge absence\n",
      "Topic 5: village muffin waffle premium plate process vicinity oil video spot\n",
      "Topic 6: habit warming octopus confusing work relax snag muffin satay envy\n",
      "Topic 7: delicacy village oil sister gym pancake confusing premium waffle dairy\n",
      "Topic 8: oil pine energy kilo conclusion whilst packet croissant village yoghurt\n",
      "Topic 9: oil premium village watermelon rigatoni energy waffle pine plate business\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab10)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(u'Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number  0\n",
      "[u'oil', u'spanish', u'village', u'plate', u'mile', u'gym', u'scale', u'employee', u'premium', u'waffle']\n",
      "Topic number  1\n",
      "[u'broccoli', u'crust', u'hazelnut', u'stand', u'veggie', u'general', u'husband', u'pal', u'brie', u'banana']\n",
      "Topic number  2\n",
      "[u'gastropub', u'package', u'pairing', u'stroll', u'marble', u'request', u'veggie', u'score', u'caf\\xe9s', u'bonus']\n",
      "Topic number  3\n",
      "[u'eating', u'research', u'crust', u'spoonful', u'shoulder', u'relaxing', u'icecream', u'pence', u'broccoli', u'design']\n",
      "Topic number  4\n",
      "[u'crust', u'spoonful', u'research', u'eating', u'broccoli', u'icecream', u'relaxing', u'pence', u'shoulder', u'travel']\n",
      "Topic number  5\n",
      "[u'research', u'crust', u'spoonful', u'eating', u'broccoli', u'relaxing', u'icecream', u'shoulder', u'pence', u'nut']\n",
      "Topic number  6\n",
      "[u'punk', u'feature', u'nut', u'kilt', u'twist', u'shellfish', u'nonsense', u'excuse', u'teeny', u'request']\n",
      "Topic number  7\n",
      "[u'rigatoni', u'fall', u'selling', u'slab', u'liver', u'hang', u'gap', u'brim', u'waste', u'doughnut']\n",
      "Topic number  8\n",
      "[u'ring', u'sit', u'critic', u'risk', u'regular', u'grease', u'demand', u'paste', u'\\xa312', u'century']\n",
      "Topic number  9\n",
      "[u'habit', u'fancy', u'warming', u'spanish', u'website', u'octopus', u'spoonful', u'component', u'confusing', u'icecream']\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(lambda_stochastic_no_ordering, vocab10, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number  0\n",
      "[u'conclusion', u'energy', u'gnocchi', u'spanish', u'charcuterie', u'twist', u'business', u'hall', u'mile', u'humus']\n",
      "Topic number  1\n",
      "[u'component', u'habit', u'spanish', u'warming', u'oil', u'mile', u'confusing', u'octopus', u'scale', u'plate']\n",
      "Topic number  2\n",
      "[u'oil', u'village', u'plate', u'spanish', u'premium', u'mile', u'muffin', u'waffle', u'gym', u'rump']\n",
      "Topic number  3\n",
      "[u'spanish', u'oil', u'delicacy', u'band', u'mile', u'plate', u'rump', u'gym', u'employee', u'scale']\n",
      "Topic number  4\n",
      "[u'oil', u'spanish', u'fancy', u'website', u'habit', u'plate', u'village', u'mile', u'employee', u'scale']\n",
      "Topic number  5\n",
      "[u'spanish', u'oil', u'habit', u'mile', u'kilo', u'scale', u'work', u'whilst', u'warming', u'octopus']\n",
      "Topic number  6\n",
      "[u'spanish', u'band', u'hash', u'oil', u'problem', u'katsu', u'mile', u'measure', u'film', u'employee']\n",
      "Topic number  7\n",
      "[u'oil', u'spanish', u'village', u'premium', u'waffle', u'mile', u'gym', u'plate', u'scale', u'employee']\n",
      "Topic number  8\n",
      "[u'spanish', u'charcuterie', u'oil', u'elephant', u'challenge', u'mile', u'brewery', u'proportion', u'voice', u'employee']\n",
      "Topic number  9\n",
      "[u'problem', u'spanish', u'meaning', u'hash', u'oil', u'scale', u'mile', u'band', u'plate', u'measure']\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(lambda_batch, vocab10, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number  0\n",
      "[u'punk', u'request', u'kilt', u'nonsense', u'teeny', u'frame', u'cheer', u'platter', u'plethora', u'reservation']\n",
      "Topic number  1\n",
      "[u'walk', u'kilo', u'travel', u'deliciousness', u'whilst', u'meze', u'croissant', u'pork', u'warmth', u'award']\n",
      "Topic number  2\n",
      "[u'rigatoni', u'broccoli', u'fall', u'selling', u'hang', u'liver', u'slab', u'gap', u'brim', u'waste']\n",
      "Topic number  3\n",
      "[u'feature', u'nut', u'twist', u'shellfish', u'excuse', u'wood', u'humus', u'shake', u'cheeseburger', u'vegetarian']\n",
      "Topic number  4\n",
      "[u'crust', u'gastropub', u'package', u'stroll', u'bonus', u'sharing', u'marble', u'\\xa315', u'outing', u'weekday']\n",
      "Topic number  5\n",
      "[u'habit', u'fancy', u'warming', u'website', u'octopus', u'spoonful', u'spanish', u'research', u'eating', u'component']\n",
      "Topic number  6\n",
      "[u'spoonful', u'research', u'eating', u'crust', u'icecream', u'shoulder', u'relaxing', u'broccoli', u'pence', u'need']\n",
      "Topic number  7\n",
      "[u'eating', u'spoonful', u'research', u'crust', u'icecream', u'relaxing', u'shoulder', u'pence', u'broccoli', u'need']\n",
      "Topic number  8\n",
      "[u'oil', u'spanish', u'plate', u'village', u'mile', u'scale', u'gym', u'employee', u'rump', u'premium']\n",
      "Topic number  9\n",
      "[u'eating', u'research', u'crust', u'spoonful', u'icecream', u'shoulder', u'pence', u'relaxing', u'broccoli', u'need']\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(lambda_stochastic, vocab10, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AP Corpus\n",
    "The models on the dataset of Edimbourgh provides a lot of redundancy, one reasion may be the high sparsity of the data. We can try our models on cleaner data set also as the AP corpus.\n",
    "The comparison need to be donne with Blei's results: \n",
    "http://www.cs.princeton.edu/~blei/lda-c/ap-topics.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the vocabulary\n",
    "ap_vocabulary = np.loadtxt('ap/vocab.txt', dtype=str)\n",
    "V = len(ap_vocabulary)\n",
    "\n",
    "# Read the data\n",
    "# Output format is a list of document (corpus) with\n",
    "# document: array([[index1, count1], ... , [index2, count2]])\n",
    "\n",
    "# To build the sparse matrix\n",
    "counts = []\n",
    "row_ind = []\n",
    "col_ind = []\n",
    "\n",
    "with open('ap/ap.dat', 'r') as f:\n",
    "    for i, row in enumerate(f):\n",
    "        # row format is:\n",
    "        #    [M] [term_1]:[count] [term_2]:[count] ...  [term_N]:[count]\n",
    "        row_raw = row.split(' ')\n",
    "        M = int(row_raw[0])\n",
    "        document = np.zeros((M, 2))\n",
    "\n",
    "        row_ind += M*[i]\n",
    "        for j, w in enumerate(row_raw[1:]):\n",
    "            document[j, :] = [int(u) for u in w.split(':')]\n",
    "        counts += list(document[:, 1])\n",
    "        col_ind += list(document[:, 0])\n",
    "\n",
    "# corpus size\n",
    "C = i + 1\n",
    "\n",
    "# Building the corpus matrix\n",
    "ap_corpus = sparse.csc_matrix((counts, (row_ind, col_ind)), shape=(C, V))\n",
    "ap_corpus = ap_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ap_corpus = ap_corpus.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choosing the number of topics\n",
    "num_topics=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 7s, sys: 3.66 s, total: 15min 11s\n",
      "Wall time: 15min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x101177638>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lda.LDA(num_topics)\n",
    "%time model.fit(ap_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 12min 31s, sys: 10min 55s, total: 2h 23min 26s\n",
      "Wall time: 2h 23min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lambda_batch, gamma_batch = batch_lda(ap_corpus, num_topics=num_topics, num_iter=100, alpha=0.001, eta=0.001, threshold=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 13min 9s, sys: 12min 28s, total: 2h 25min 37s\n",
      "Wall time: 2h 25min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lambda_stochastic, gamma_stochastic = stochastic_lda_ordering(ap_corpus, S=10, num_topics=num_topics, max_iter=100, tau=500, kappa=0.6, alpha=0.001, eta=0.001, threshold=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-5da893be6ed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'lambda_stochastic_no_ordering, gamma_stochastic_no_ordering = stochastic_lda(ap_corpus, S=10, num_topics=num_topics, max_iter=100, tau=500, kappa=0.6, alpha=0.001, eta=0.001, threshold=0.000001)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-b6eb5ea5e8c8>\u001b[0m in \u001b[0;36mstochastic_lda\u001b[0;34m(corpus, batches, lambda_, ordering, S, num_topics, max_iter, tau, kappa, alpha, eta, threshold)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mgamma_d_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_d_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# #### M-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-856ae95f774f>\u001b[0m in \u001b[0;36me_step\u001b[0;34m(d, corpus, gamma_d_k, lambda_, lambda_int, alpha, threshold)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgamma_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma_d_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mE_log_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdigamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: Wait for convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Used to check convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-856ae95f774f>\u001b[0m in \u001b[0;36mdigamma\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lambda_stochastic_no_ordering, gamma_stochastic_no_ordering = stochastic_lda(ap_corpus, S=10, num_topics=num_topics, max_iter=100, tau=500, kappa=0.6, alpha=0.001, eta=0.001, threshold=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number  0\n",
      "['fleisher', 'blackburn', 'pomona', 'leek', 'shuster', 'beazley', 'amirav', 'transistor', 'zennoh', 'bordallo']\n",
      "Topic number  1\n",
      "['bordallo', 'sowan', 'morgenstern', 'blackburn', 'quaker', 'hasselbring', 'sheftel', 'ticketron', 'anasazi', 'lobban']\n",
      "Topic number  2\n",
      "['coasters', 'leek', 'barbershop', 'quints', 'popeye', 'tartus', 'wallenda', 'hildreths', 'shiley', 'buckey']\n",
      "Topic number  3\n",
      "['harwood', 'bst', 'patmos', 'pekin', 'hastings', 'gaviria', 'bordallo', 'teeley', 'chatham', 'menem']\n",
      "Topic number  4\n",
      "['coasters', 'galileo', 'alpo', 'seidon', 'southwell', 'lukanov', 'guterman', 'thyssen', 'transistor', 'poe']\n",
      "Topic number  5\n",
      "['storks', 'blackburn', 'turtles', 'ganyile', 'hazelwood', 'mpaa', 'chatham', 'fleisher', 'etruscan', 'dalai']\n",
      "Topic number  6\n",
      "['gisclair', 'quints', 'corkscrew', 'chiquita', 'kolb', 'erols', 'dalai', 'transistor', 'kiesner', 'brides']\n",
      "Topic number  7\n",
      "['etruscan', 'mydland', 'leek', 'bridesmaids', 'jal', 'herons', 'putnam', 'erols', 'blackowned', 'pomona']\n",
      "Topic number  8\n",
      "['cdy', 'clr', 'rn', 'calgary', 'paulo', 'stockholm', 'budapest', 'lisbon', 'vancouver', 'taipei']\n",
      "Topic number  9\n",
      "['cosell', 'kephart', 'quaker', 'pauley', 'bosch', 'coasters', 'storks', 'collider', 'glauberman', 'joyes']\n",
      "Topic number  10\n",
      "['shiley', 'galileo', 'castaneda', 'bridesmaids', 'ienner', 'patmos', 'minnick', 'furmark', 'massage', 'hoppe']\n",
      "Topic number  11\n",
      "['mecham', 'steiger', 'patriarca', 'milstead', 'hastings', 'sedlmayr', 'keating', 'deconcini', 'eckstein', 'mofford']\n",
      "Topic number  12\n",
      "['patmos', 'corroon', 'sheftel', 'leek', 'coasters', 'algotson', 'kolb', 'gardner', 'quints', 'ornellas']\n",
      "Topic number  13\n",
      "['ienner', 'telecharge', 'kephart', 'patmos', 'wiese', 'galileos', 'nebinger', 'hasselbring', 'collider', 'faber']\n",
      "Topic number  14\n",
      "['putnam', 'galileo', 'phobos', 'mayer', 'scherer', 'faber', 'hazelwood', 'transistor', 'tartus', 'eckstein']\n",
      "Topic number  15\n",
      "['anasazi', 'pomona', 'corkscrew', 'patmos', 'hoppe', 'menem', 'quints', 'manger', 'trehan', 'siegelman']\n",
      "Topic number  16\n",
      "['liberace', 'bloomberg', 'sheftel', 'ienner', 'leek', 'guida', 'budvar', 'castaneda', 'montezumas', 'beazley']\n",
      "Topic number  17\n",
      "['joyes', 'patmos', 'lukanov', 'menendez', 'depardieu', 'anasazi', 'phobos', 'pomona', 'montezumas', 'herons']\n",
      "Topic number  18\n",
      "['hubbert', 'greyhound', 'friedrick', 'souter', 'siegelman', 'feeney', 'rios', 'batalla', 'chiquita', 'kalugin']\n",
      "Topic number  19\n",
      "['back', 'liberace', 'bloomberg', 'montezumas', 'aon', 'plessey', 'blackburn', 'pomona', 'leek', 'nikolais']\n",
      "Topic number  20\n",
      "['anasazi', 'collider', 'corroon', 'narez', 'zennoh', 'telecharge', 'erols', 'galileo', 'buckey', 'mumford']\n",
      "Topic number  21\n",
      "['feazell', 'leek', 'wiese', 'nikolais', 'etruscan', 'dickman', 'chiquita', 'burleson', 'galileos', 'monia']\n",
      "Topic number  22\n",
      "['amirav', 'baguio', 'narez', 'ticketron', 'kolb', 'tremolite', 'zennoh', 'wilburys', 'galileos', 'faber']\n",
      "Topic number  23\n",
      "['manger', 'fruehauf', 'thyssen', 'zennoh', 'boharski', 'chatham', 'blockbuster', 'forman', 'pomona', 'gisclair']\n",
      "Topic number  24\n",
      "['boharski', 'menem', 'erols', 'nikolais', 'wynberg', 'storks', 'rajneesh', 'brides', 'gardner', 'rabuka']\n",
      "Topic number  25\n",
      "['gotti', 'ticketron', 'wynberg', 'toussaint', 'getz', 'telecharge', 'gesell', 'zubal', 'cosell', 'teletron']\n",
      "Topic number  26\n",
      "['chafee', 'burleson', 'kiesner', 'patmos', 'wilburys', 'niklus', 'viett', 'boharski', 'amirav', 'kephart']\n",
      "Topic number  27\n",
      "['sheftel', 'feeney', 'viett', 'sedlmayr', 'mpaa', 'blackowned', 'massage', 'holderman', 'feazell', 'willis']\n",
      "Topic number  28\n",
      "['leek', 'liberace', 'anasazi', 'montezumas', 'bloomberg', 'narez', 'blackburn', 'coasters', 'gunter', 'presleys']\n",
      "Topic number  29\n",
      "['patmos', 'quints', 'lundgren', 'bosch', 'chafee', 'kephart', 'coasters', 'kiesner', 'massage', 'collider']\n",
      "Topic number  30\n",
      "['pring', 'bridesmaids', 'quints', 'harwood', 'mpaa', 'transistor', 'ornellas', 'travell', 'galileos', 'fruehauf']\n",
      "Topic number  31\n",
      "['coasters', 'telecharge', 'lundgren', 'bridesmaids', 'bst', 'eclipses', 'galileos', 'lukanov', 'warmus', 'menendez']\n",
      "Topic number  32\n",
      "['wynberg', 'wiese', 'corroon', 'nikolais', 'wilburys', 'zennoh', 'telecharge', 'kolb', 'dennys', 'khashoggi']\n",
      "Topic number  33\n",
      "['castaneda', 'khashoggi', 'ganyile', 'montezumas', 'mumford', 'daisy', 'phobos', 'coasters', 'telecharge', 'ziemet']\n",
      "Topic number  34\n",
      "['blackowned', 'patmos', 'montezumas', 'odom', 'storks', 'mccown', 'letterman', 'bordallo', 'castaneda', 'branover']\n",
      "Topic number  35\n",
      "['galileo', 'demjanjuk', 'sheftel', 'venus', 'sinhalese', 'hezbollah', 'magellan', 'phobos', 'oberg', 'sikh']\n",
      "Topic number  36\n",
      "['plessey', 'corkscrew', 'phobos', 'manger', 'mayer', 'puette', 'daisy', 'dennys', 'wilburys', 'pillsburys']\n",
      "Topic number  37\n",
      "['storks', 'galileo', 'daisy', 'corkscrew', 'patmos', 'smeal', 'blier', 'wallenda', 'dyke', 'montezumas']\n",
      "Topic number  38\n",
      "['polly', 'patmos', 'laurentiis', 'quaker', 'barbershop', 'storks', 'boharski', 'scherer', 'teletron', 'manger']\n",
      "Topic number  39\n",
      "['furmark', 'puette', 'leek', 'tannery', 'burleson', 'rajneesh', 'thyssen', 'dennys', 'harwood', 'hasselbring']\n",
      "Topic number  40\n",
      "['ganyile', 'wallenda', 'wiese', 'poe', 'kephart', 'bloomberg', 'quints', 'khashoggi', 'fleisher', 'patmos']\n",
      "Topic number  41\n",
      "['tremolite', 'pomona', 'branover', 'leek', 'rabuka', 'pillsburys', 'cna', 'dennys', 'wynberg', 'wte']\n",
      "Topic number  42\n",
      "['lukanov', 'guterman', 'hazelwood', 'narez', 'centrust', 'branover', 'erols', 'ganyile', 'schneidman', 'hildreth']\n",
      "Topic number  43\n",
      "['turtles', 'edgemont', 'pekin', 'transistor', 'mussels', 'tannery', 'ganyile', 'herrington', 'romer', 'storks']\n",
      "Topic number  44\n",
      "['pang', 'sheftel', 'seidon', 'etruscan', 'montezumas', 'coasters', 'nikolais', 'canning', 'mayer', 'bst']\n",
      "Topic number  45\n",
      "['wilburys', 'storks', 'rabuka', 'ornellas', 'brides', 'wiese', 'blier', 'willis', 'amirav', 'pekin']\n",
      "Topic number  46\n",
      "['etruscan', 'sowan', 'burleson', 'teletron', 'narez', 'shiley', 'ienner', 'mears', 'zennoh', 'laurentiis']\n",
      "Topic number  47\n",
      "['wiese', 'quints', 'shiley', 'bordallo', 'manger', 'fruehauf', 'brigades', 'kolb', 'hasselbring', 'gardner']\n",
      "Topic number  48\n",
      "['fruehauf', 'radakovich', 'koppers', 'oferrell', 'busfield', 'eclipses', 'coasters', 'teletron', 'polly', 'icc']\n",
      "Topic number  49\n",
      "['corkscrew', 'montezumas', 'chatham', 'sedlmayr', 'joyes', 'buckey', 'willis', 'cerullo', 'trehan', 'fleisher']\n",
      "Topic number  50\n",
      "['bcspehealth', 'b', 'cereal', 'cholesterol', 'arthritis', 'schroeder', 'videos', 'sickness', 'anxiety', 'nutrition']\n",
      "Topic number  51\n",
      "['seidon', 'mussels', 'brides', 'bosch', 'coasters', 'fijians', 'mayer', 'niklus', 'ganyile', 'joyes']\n",
      "Topic number  52\n",
      "['hazelwood', 'patmos', 'thyssen', 'castaneda', 'depardieu', 'shiley', 'fijians', 'leek', 'dalai', 'dna']\n",
      "Topic number  53\n",
      "['liberace', 'bloomberg', 'harwood', 'kiesner', 'leek', 'fis', 'lukanov', 'ticketron', 'etruscan', 'vase']\n",
      "Topic number  54\n",
      "['pang', 'centrust', 'sigmond', 'ienner', 'kiesner', 'lukanov', 'rn', 'toussaint', 'wppss', 'sheftel']\n",
      "Topic number  55\n",
      "['ehrenhalt', 'hazelwood', 'lukanov', 'phobos', 'mccown', 'vase', 'ganyile', 'putnam', 'vanuatu', 'arc']\n",
      "Topic number  56\n",
      "['transistor', 'mccown', 'kiesner', 'lukanov', 'warmus', 'leek', 'montezumas', 'gracyalny', 'symchych', 'narez']\n",
      "Topic number  57\n",
      "['transistor', 'telecharge', 'corkscrew', 'wppss', 'fruehauf', 'gisclair', 'gruber', 'guterman', 'ticketron', 'popeye']\n",
      "Topic number  58\n",
      "['transistor', 'katyn', 'chiquita', 'montezumas', 'polly', 'faber', 'chatham', 'ienner', 'herons', 'sassan']\n",
      "Topic number  59\n",
      "['corkscrew', 'brides', 'blackburn', 'ehrenhalt', 'khashoggi', 'southwell', 'menem', 'wilburys', 'feazell', 'bloomberg']\n",
      "Topic number  60\n",
      "['harwood', 'bridesmaids', 'dalai', 'kephart', 'vanuatu', 'montezumas', 'wilburys', 'teletron', 'siegelman', 'pauley']\n",
      "Topic number  61\n",
      "['nordstrom', 'warmus', 'hildreth', 'oferrell', 'minnick', 'gacy', 'menorah', 'guida', 'bricklin', 'zaccaro']\n",
      "Topic number  62\n",
      "['storks', 'brides', 'ticketron', 'odom', 'hasegawa', 'abboud', 'herons', 'mayer', 'shiley', 'coasters']\n",
      "Topic number  63\n",
      "['lindsey', 'faithful', 'mash', 'brush', 'verne', 'knudsen', 'fires', 'circles', 'rings', 'forest']\n",
      "Topic number  64\n",
      "['sedlmayr', 'blackburn', 'narez', 'bosch', 'leek', 'telecharge', 'menendez', 'mayer', 'warmus', 'hasselbring']\n",
      "Topic number  65\n",
      "['ames', 'polaroid', 'chinn', 'shamrock', 'corroon', 'sipc', 'duracell', 'coleco', 'greenwald', 'centrust']\n",
      "Topic number  66\n",
      "['sheftel', 'storks', 'polly', 'sowan', 'duracell', 'documentmatching', 'fruehauf', 'blackowned', 'wiese', 'anasazi']\n",
      "Topic number  67\n",
      "['gardner', 'riggs', 'massage', 'eclipses', 'popeye', 'bosch', 'menendez', 'fleisher', 'mydland', 'hasselbring']\n",
      "Topic number  68\n",
      "['minnick', 'duracell', 'wiese', 'fijian', 'chafee', 'ornellas', 'kephart', 'batalla', 'beazley', 'corkscrew']\n",
      "Topic number  69\n",
      "['cosell', 'leek', 'harwood', 'zennoh', 'gaviria', 'schneidman', 'aon', 'depardieu', 'pring', 'narez']\n",
      "Topic number  70\n",
      "['transistor', 'bosch', 'wilburys', 'massage', 'quaker', 'montt', 'putnam', 'chatham', 'lasko', 'faber']\n",
      "Topic number  71\n",
      "['spoor', 'corroon', 'fijian', 'schneidman', 'ienner', 'blockbuster', 'phobos', 'kenner', 'hastings', 'dna']\n",
      "Topic number  72\n",
      "['tremolite', 'seidon', 'wppss', 'toussaint', 'patmos', 'manger', 'gracyalny', 'anasazi', 'narez', 'shiley']\n",
      "Topic number  73\n",
      "['batalla', 'alpo', 'southwell', 'brides', 'menendez', 'zennoh', 'etruscan', 'castaneda', 'radakovich', 'blackburn']\n",
      "Topic number  74\n",
      "['shiley', 'gacy', 'narez', 'bridesmaids', 'bordallo', 'wte', 'alpo', 'viett', 'wilburys', 'chia']\n",
      "Topic number  75\n",
      "['thyssen', 'leek', 'lukanov', 'berrigan', 'canning', 'massage', 'dennys', 'brides', 'blier', 'wiese']\n",
      "Topic number  76\n",
      "['menendez', 'alpo', 'storks', 'daisy', 'musburger', 'eckstein', 'shiley', 'mccown', 'massage', 'menem']\n",
      "Topic number  77\n",
      "['menendez', 'etruscan', 'thyssen', 'forman', 'leek', 'menem', 'wallenda', 'cdy', 'toussaint', 'ganyile']\n",
      "Topic number  78\n",
      "['wilburys', 'mpaa', 'corkscrew', 'phobos', 'holyoke', 'algotson', 'spoor', 'telecharge', 'zennoh', 'southwell']\n",
      "Topic number  79\n",
      "['cdy', 'm', 'clr', 'infantry', 'saudi', 'manning', 'mechanized', 'tanks', 'armor', 'speedy']\n",
      "Topic number  80\n",
      "['lukanov', 'teeley', 'bedard', 'pomona', 'collider', 'ticketron', 'brigades', 'hazelwood', 'chafee', 'mayer']\n",
      "Topic number  81\n",
      "['southwell', 'dna', 'chiquita', 'phobos', 'aon', 'brides', 'galileos', 'fleisher', 'shiley', 'menem']\n",
      "Topic number  82\n",
      "['buenos', 'aires', 'puette', 'transistor', 'hazelwood', 'alpo', 'steiger', 'rabuka', 'gardner', 'dalai']\n",
      "Topic number  83\n",
      "['ienner', 'montt', 'anasazi', 'montezumas', 'cuito', 'trehan', 'galileo', 'ganyile', 'bridesmaids', 'patmos']\n",
      "Topic number  84\n",
      "['shuster', 'sakharov', 'menem', 'grigoryants', 'niklus', 'schwarzkopf', 'algotson', 'castaneda', 'lukanov', 'cnn']\n",
      "Topic number  85\n",
      "['blackburn', 'galileos', 'bosch', 'lukanov', 'chatham', 'faber', 'vase', 'narez', 'receptor', 'joyes']\n",
      "Topic number  86\n",
      "['anasazi', 'puette', 'shiley', 'travell', 'mears', 'broyles', 'mayer', 'tartus', 'galileo', 'galileos']\n",
      "Topic number  87\n",
      "['menendez', 'leek', 'bst', 'fijians', 'ienner', 'wte', 'corroon', 'budvar', 'cna', 'shiley']\n",
      "Topic number  88\n",
      "['ganyile', 'chiquita', 'etruscan', 'bridesmaids', 'vanuatu', 'creque', 'viett', 'corroon', 'montezumas', 'corkscrew']\n",
      "Topic number  89\n",
      "['fruehauf', 'leek', 'poe', 'daisy', 'gisclair', 'narez', 'cerullo', 'galileos', 'anasazi', 'dennys']\n",
      "Topic number  90\n",
      "['quints', 'menem', 'galileo', 'siegelman', 'collider', 'bedard', 'patmos', 'swindler', 'ornellas', 'warmus']\n",
      "Topic number  91\n",
      "['transistor', 'southwell', 'patmos', 'fruehauf', 'rabuka', 'faber', 'gardner', 'anasazi', 'etruscan', 'harwood']\n",
      "Topic number  92\n",
      "['corkscrew', 'teletron', 'ganyile', 'wynberg', 'patmos', 'pomona', 'bridesmaids', 'chatham', 'dna', 'coasters']\n",
      "Topic number  93\n",
      "['brides', 'pomona', 'ienner', 'greenwald', 'baugh', 'mattox', 'etruscan', 'shamrock', 'hazelwood', 'southwell']\n",
      "Topic number  94\n",
      "['kephart', 'leek', 'koppers', 'menem', 'storks', 'sheftel', 'gardner', 'dyke', 'galileo', 'patmos']\n",
      "Topic number  95\n",
      "['ganyile', 'mayer', 'ienner', 'putnam', 'pang', 'phobos', 'anasazi', 'forman', 'steiger', 'alpo']\n",
      "Topic number  96\n",
      "['nauvoo', 'bono', 'southwell', 'fleisher', 'peewee', 'vase', 'broyles', 'yates', 'blier', 'popeye']\n",
      "Topic number  97\n",
      "['kephart', 'bordallo', 'galileos', 'jal', 'trehan', 'teletron', 'leek', 'dickman', 'brigades', 'shuster']\n",
      "Topic number  98\n",
      "['timbuktu', 'chafee', 'hazelwood', 'galileos', 'shiley', 'morgenstern', 'pomona', 'rinfret', 'wynberg', 'hildreth']\n",
      "Topic number  99\n",
      "['aspirin', 'perrys', 'schneidman', 'wallenda', 'dna', 'trehan', 'breast', 'jarvik', 'bst', 'nida']\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(lambda_stochastic, vocabulary, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number  0\n",
      "['million', 'network', 'bush', 'president', 'aristide', 'skull', 'bones', 'television', 'cnn', 'ptl']\n",
      "Topic number  1\n",
      "['american', 'i', 'new', 'gotti', 'blood', 'blier', 'says', 'filed', 'people', 'year']\n",
      "Topic number  2\n",
      "['i', 'rights', 'grigoryants', 'new', 'last', 'southern', 'year', 'force', 'authority', 'committee']\n",
      "Topic number  3\n",
      "['police', 'people', 'force', 'air', 'government', 'two', 'city', 'killed', 'today', 'thursday']\n",
      "Topic number  4\n",
      "['mall', 'downtown', 'report', 'lincoln', 'communist', 'cities', 'senators', 'malls', 'group', 'released']\n",
      "Topic number  5\n",
      "['i', 'people', 'mrs', 'police', 'years', 'president', 'two', 'three', 'government', 'think']\n",
      "Topic number  6\n",
      "['year', 'company', 'police', 'two', 'chinn', 'three', 'meese', 'men', 'record', 'years']\n",
      "Topic number  7\n",
      "['northern', 'hair', 'training', 'dna', 'temperatures', 'new', 'family', 'count', 'band', 'increase']\n",
      "Topic number  8\n",
      "['estate', 'mrs', 'meese', 'home', 'property', 'people', 'tax', 'federal', 'last', 'mine']\n",
      "Topic number  9\n",
      "['soviet', 'air', 'uta', 'france', 'percent', 'troops', 'paris', 'vase', 'people', 'guerrillas']\n",
      "Topic number  10\n",
      "['reagan', 'last', 'percent', 'president', 'market', 'house', 'first', 'news', 'people', 'company']\n",
      "Topic number  11\n",
      "['creek', 'new', 'officials', 'state', 'drug', 'first', 'says', 'arafat', 'years', 'north']\n",
      "Topic number  12\n",
      "['police', 'school', 'tons', 'people', 'spokesman', 'i', 'plant', 'officials', 'president', 'gunmen']\n",
      "Topic number  13\n",
      "['dollar', 'late', 'rates', 'yen', 'bid', 'london', 'gold', 'price', 'rose', 'compared']\n",
      "Topic number  14\n",
      "['i', 'people', 'think', 'bush', 'government', 'thats', 'told', 'congress', 'new', 'president']\n",
      "Topic number  15\n",
      "['government', 'party', 'new', 'national', 'percent', 'year', 'two', 'economy', 'three', 'last']\n",
      "Topic number  16\n",
      "['iraq', 'people', 'new', 'lower', 'president', 'war', 'peace', 'officials', 'talks', 'minister']\n",
      "Topic number  17\n",
      "['inches', 'people', 'state', 'central', 'wednesday', 'snow', 'ohio', 'rain', 'county', 'high']\n",
      "Topic number  18\n",
      "['bush', 'million', 'new', 'year', 'president', 'campaign', 'state', 'percent', 'republican', 'i']\n",
      "Topic number  19\n",
      "['soviet', 'united', 'states', 'two', 'officials', 'space', 'military', 'shuttle', 'american', 'soviets']\n",
      "Topic number  20\n",
      "['trade', 'percent', 'united', 'billion', 'states', 'bill', 'dollar', 'farm', 'bush', 'west']\n",
      "Topic number  21\n",
      "['pilots', 'eastern', 'air', 'airlines', 'orion', 'aviation', 'faa', 'dresses', 'accident', 'accidents']\n",
      "Topic number  22\n",
      "['people', 'demjanjuk', 'today', 'i', 'computer', 'eight', 'two', 'told', 'death', 'investigation']\n",
      "Topic number  23\n",
      "['mecham', 'state', 'office', 'ms', 'union', 'southwell', 'senate', 'two', 'threat', 'milstead']\n",
      "Topic number  24\n",
      "['cents', 'million', 'lower', 'cent', 'higher', 'soybean', 'corn', 'government', 'bushel', 'futures']\n",
      "Topic number  25\n",
      "['care', 'house', 'aid', 'people', 'b', 'bcspehealth', 'years', 'day', 'new', 'year']\n",
      "Topic number  26\n",
      "['police', 'i', 'tuesday', 'two', 'prison', 'ms', 'president', 'day', 'told', 'years']\n",
      "Topic number  27\n",
      "['million', 'macmillan', 'percent', 'students', 'maxwell', 'billion', 'members', 'share', 'milken', 'year']\n",
      "Topic number  28\n",
      "['i', 'billion', 'two', 'year', 'insurance', 'water', 'federal', 'time', 'children', 'percent']\n",
      "Topic number  29\n",
      "['i', 'new', 'two', 'iran', 'year', 'state', 'santa', 'years', 'california', 'officials']\n",
      "Topic number  30\n",
      "['army', 'government', 'people', 'death', 'killed', 'years', 'court', 'rep', 'two', 'police']\n",
      "Topic number  31\n",
      "['state', 'turkish', 'million', 'say', 'medication', 'perrys', 'government', 'circles', 'perry', 'court']\n",
      "Topic number  32\n",
      "['workers', 'contract', 'union', 'company', 'strike', 'polaroid', 'shamrock', 'breeze', 'board', 'chrysler']\n",
      "Topic number  33\n",
      "['sales', 'percent', 'company', 'new', 'market', 'number', 'analysts', 'board', 'billion', 'duracell']\n",
      "Topic number  34\n",
      "['soviet', 'american', 'steel', 'frank', 'i', 'summer', 'policy', 'transportation', 'committee', 'trade']\n",
      "Topic number  35\n",
      "['east', 'german', 'west', 'billion', 'germany', 'defense', 'budget', 'new', 'military', 'germans']\n",
      "Topic number  36\n",
      "['i', 'people', 'department', 'years', 'million', 'program', 'two', 'new', 'dont', 'water']\n",
      "Topic number  37\n",
      "['million', 'company', 'nordstrom', 'food', 'sales', 'officials', 'jackpot', 'i', 'president', 'convoy']\n",
      "Topic number  38\n",
      "['israel', 'union', 'government', 'soviet', 'new', 'radio', 'billion', 'foreign', 'states', 'united']\n",
      "Topic number  39\n",
      "['like', 'years', 'people', 'year', 'hospital', 'pageant', 'new', 'service', 'baby', 'million']\n",
      "Topic number  40\n",
      "['i', 'bush', 'dukakis', 'market', 'president', 'stock', 'american', 'new', 'two', 'today']\n",
      "Topic number  41\n",
      "['two', 'aid', 'i', 'government', 'military', 'rebels', 'contras', 'police', 'house', 'agreement']\n",
      "Topic number  42\n",
      "['soviet', 'gorbachev', 'party', 'united', 'president', 'government', 'germany', 'states', 'union', 'west']\n",
      "Topic number  43\n",
      "['jewish', 'letter', 'israel', 'eastern', 'wellstone', 'officials', 'bonds', 'drexel', 'eggs', 'employees']\n",
      "Topic number  44\n",
      "['percent', 'i', 'year', 'last', 'million', 'month', 'government', 'index', 'house', 'trading']\n",
      "Topic number  45\n",
      "['president', 'year', 'years', 'reagan', 'new', 'million', 'people', 'military', 'nato', 'first']\n",
      "Topic number  46\n",
      "['soviet', 'people', 'i', 'president', 'new', 'state', 'union', 'government', 'told', 'states']\n",
      "Topic number  47\n",
      "['bank', 'cents', 'new', 'hunt', 'plan', 'police', 'year', 'hubbert', 'house', 'state']\n",
      "Topic number  48\n",
      "['percent', 'new', 'city', 'last', 'year', 'workers', 'council', 'labor', 'prime', 'court']\n",
      "Topic number  49\n",
      "['percent', 'stock', 'market', 'new', 'million', 'rose', 'prices', 'index', 'year', 'billion']\n",
      "Topic number  50\n",
      "['aids', 'show', 'game', 'american', 'patients', 'americans', 'health', 'white', 'humphrey', 'evening']\n",
      "Topic number  51\n",
      "['soviet', 'today', 'war', 'court', 'union', 'time', 'president', 'polish', 'officers', 'inmates']\n",
      "Topic number  52\n",
      "['police', 'i', 'hospital', 'two', 'told', 'home', 'people', 'last', 'war', 'killed']\n",
      "Topic number  53\n",
      "['new', 'abortion', 'life', 'soviet', 'state', 'ec', 'church', 'mexico', 'last', 'toussaint']\n",
      "Topic number  54\n",
      "['house', 'people', 'new', 'noriega', 'i', 'panama', 'officials', 'states', 'bill', 'time']\n",
      "Topic number  55\n",
      "['jackson', 'new', 'dukakis', 'fire', 'i', 'house', 'first', 'primary', 'democratic', 'york']\n",
      "Topic number  56\n",
      "['states', 'vote', 'people', 'government', 'year', 'two', 'last', 'national', 'kuwait', 'israel']\n",
      "Topic number  57\n",
      "['shultz', 'police', 'nixon', 'peace', 'shamir', 'president', 'two', 'ruth', 'workers', 'international']\n",
      "Topic number  58\n",
      "['company', 'plant', 'new', 'farmers', 'workers', 'chrysler', 'president', 'friday', 'state', 'island']\n",
      "Topic number  59\n",
      "['million', 'homes', 'people', 'schneidman', 'mother', 'last', 'nbc', 'years', 'fall', 'house']\n",
      "Topic number  60\n",
      "['percent', 'state', 'sales', 'two', 'year', 'price', 'customers', 'federal', 'judge', 'domestic']\n",
      "Topic number  61\n",
      "['company', 'million', 'stock', 'department', 'year', 'two', 'tuesday', 'reform', 'first', 'debt']\n",
      "Topic number  62\n",
      "['i', 'new', 'people', 'children', 'years', 'million', 'government', 'percent', 'say', 'number']\n",
      "Topic number  63\n",
      "['trade', 'japan', 'japanese', 'states', 'united', 'agreement', 'percent', 'last', 'beef', 'government']\n",
      "Topic number  64\n",
      "['south', 'africa', 'states', 'united', 'african', 'two', 'government', 'north', 'officials', 'black']\n",
      "Topic number  65\n",
      "['government', 'school', 'rebels', 'president', 'vote', 'peace', 'percent', 'two', 'billion', 'party']\n",
      "Topic number  66\n",
      "['reed', 'students', 'school', 'northwest', 'state', 'flight', 'teachers', 'made', 'two', 'federal']\n",
      "Topic number  67\n",
      "['i', 'united', 'available', 'percent', 'number', 'committee', 'minister', 'report', 'talks', 'trade']\n",
      "Topic number  68\n",
      "['air', 'navy', 'art', 'force', 'algotson', 'works', 'iraqi', 'united', 'last', 'thieves']\n",
      "Topic number  69\n",
      "['year', 'tax', 'people', 'million', 'bush', 'last', 'parents', 'president', 'years', 'children']\n",
      "Topic number  70\n",
      "['iraq', 'today', 'iraqi', 'people', 'magellan', 'united', 'two', 'report', 'office', 'war']\n",
      "Topic number  71\n",
      "['city', 'inheritance', 'new', 'i', 'york', 'church', 'million', 'condoms', 'inc', 'john']\n",
      "Topic number  72\n",
      "['rep', 'wine', 'drug', 'games', 'tests', 'service', 'members', 'workers', 'last', 'olympic']\n",
      "Topic number  73\n",
      "['bush', 'budget', 'first', 'cbs', 'week', 'percent', 'new', 'president', 'nbc', 'news']\n",
      "Topic number  74\n",
      "['oil', 'show', 'opec', 'new', 'two', 'musical', 'york', 'lukman', 'state', 'ministers']\n",
      "Topic number  75\n",
      "['percent', 'million', 'new', 'party', 'convention', 'bush', 'campaign', 'president', 'republican', 'bill']\n",
      "Topic number  76\n",
      "['i', 'barry', 'american', 'think', 'dont', 'first', 'million', 'asked', 'just', 'black']\n",
      "Topic number  77\n",
      "['million', 'trust', 'claims', 'venus', 'manville', 'asbestos', 'plan', 'last', 'payments', 'galileo']\n",
      "Topic number  78\n",
      "['bill', 'legislation', 'senate', 'board', 'million', 'agreement', 'housing', 'house', 'air', 'federal']\n",
      "Topic number  79\n",
      "['fair', 'northern', 'rain', 'new', 'snow', 'refuge', 'jal', 'coast', 'power', 'high']\n",
      "Topic number  80\n",
      "['percent', 'billion', 'bushels', 'last', 'new', 'planes', 'tunnel', 'production', 'year', 'solidarity']\n",
      "Topic number  81\n",
      "['government', 'million', 'opposition', 'robertson', 'premier', 'monday', 'coast', 'new', 'systems', 'president']\n",
      "Topic number  82\n",
      "['i', 'west', 'two', 'bank', 'kephart', 'system', 'ruby', 'monet', 'thought', 'million']\n",
      "Topic number  83\n",
      "['drug', 'news', 'new', 'medellin', 'i', 'people', 'years', 'federal', 'national', 'futures']\n",
      "Topic number  84\n",
      "['percent', 'year', 'million', 'years', 'rate', 'sales', 'business', 'two', 'billion', 'quarter']\n",
      "Topic number  85\n",
      "['fair', 'court', 'trial', 'testimony', 'cloudy', 'case', 'evidence', 'north', 'judge', 'irancontra']\n",
      "Topic number  86\n",
      "['new', 'm', 'i', 'government', 'years', 'two', 'cdy', 'court', 'federal', 'clr']\n",
      "Topic number  87\n",
      "['new', 'dollar', 'late', 'students', 'york', 'president', 'yen', 'friday', 'fell', 'bid']\n",
      "Topic number  88\n",
      "['oil', 'cents', 'cent', 'futures', 'crude', 'higher', 'lower', 'gasoline', 'new', 'gallon']\n",
      "Topic number  89\n",
      "['percent', 'year', 'new', 'i', 'report', 'last', 'federal', 'million', 'drug', 'owen']\n",
      "Topic number  90\n",
      "['i', 'de', 'question', 'united', 'talks', 'heart', 'states', 'klerk', 'president', 'un']\n",
      "Topic number  91\n",
      "['billion', 'percent', 'year', 'million', 'rose', 'quarter', 'ira', 'northwest', 'sales', 'debt']\n",
      "Topic number  92\n",
      "['strike', 'union', 'police', 'news', 'heat', 'contract', 'workers', 'crime', 'water', 'department']\n",
      "Topic number  93\n",
      "['police', 'people', 'state', 'new', 'million', 'killed', 'fire', 'two', 'last', 'officials']\n",
      "Topic number  94\n",
      "['new', 'i', 'nurses', 'hospitals', 'work', 'government', 'officials', 'two', 'hale', 'contract']\n",
      "Topic number  95\n",
      "['party', 'percent', 'government', 'mrs', 'people', 'new', 'i', 'elections', 'years', 'three']\n",
      "Topic number  96\n",
      "['percent', 'market', 'points', 'prices', 'documents', 'yen', 'point', 'north', 'government', 'stock']\n",
      "Topic number  97\n",
      "['police', 'fire', 'south', 'national', 'bentsen', 'people', 'i', 'tuesday', 'two', 'west']\n",
      "Topic number  98\n",
      "['dukakis', 'bush', 'percent', 'president', 'poll', 'million', 'campaign', 'year', 'states', 'democratic']\n",
      "Topic number  99\n",
      "['court', 'million', 'oil', 'federal', 'i', 'drug', 'last', 'two', 'three', 'rights']\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(lambda_batch, vocabulary, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: party political elections opposition government election minister vote parties first\n",
      "Topic 1: military rebels government president guerrillas coup last human war civil\n",
      "Topic 2: people india indian government state percent million militants population pakistan\n",
      "Topic 3: prison death prisoners years released release state day convicted human\n",
      "Topic 4: hostages lebanon mecham beirut christian hezbollah today syrian hijackers held\n",
      "Topic 5: church catholic pope john religious roman paul vatican bishops rev\n",
      "Topic 6: court judge case filed supreme ruling federal appeals order lawsuit\n",
      "Topic 7: lincoln senators five keating regulators deconcini senate dennis jr morris\n",
      "Topic 8: south africa black african government mandela de blacks national apartheid\n",
      "Topic 9: trial charges court case judge guilty attorney jury prison convicted\n",
      "Topic 10: workers union contract labor strike jobs employees work company unions\n",
      "Topic 11: world war th american ago years history ii president today\n",
      "Topic 12: stores store business sales christmas chain tuesday company nordstrom plans\n",
      "Topic 13: oil prices price gas energy gasoline production opec day friday\n",
      "Topic 14: killed army soldiers troops police violence forces miles wounded fighting\n",
      "Topic 15: stock index market stocks points rose shares trading million exchange\n",
      "Topic 16: computer research systems work test technology system testing scientists data\n",
      "Topic 17: i get like dont people going just time think thats\n",
      "Topic 18: i man got right war went just show vietnam im\n",
      "Topic 19: king found ray time nixon animals two martin put animal\n",
      "Topic 20: i think dont know people going told asked say just\n",
      "Topic 21: building office center site hotel city officials construction moved buildings\n",
      "Topic 22: new york city jersey boston regional bradley cuomo planned week\n",
      "Topic 23: san california los angeles francisco santa calif friday texas county\n",
      "Topic 24: show roberts white williams available evening number young dresses looked\n",
      "Topic 25: soviet gorbachev party communist republics tass union gorbachevs political reforms\n",
      "Topic 26: firm securities trading futures smith stock investment inc new market\n",
      "Topic 27: new economic plan help government system policy take change economy\n",
      "Topic 28: trade japan states united japanese world countries international billion american\n",
      "Topic 29: year last first months july april wednesday month three june\n",
      "Topic 30: report program public federal problems required general made says final\n",
      "Topic 31: rates market interest prices week point analysts economy economic federal\n",
      "Topic 32: office investigation attorney general department justice meese information last staff\n",
      "Topic 33: southern fair rain northern inches high snow temperatures degrees heat\n",
      "Topic 34: food town royal table picture family like palace small eight\n",
      "Topic 35: mrs family wife home yearold years i life son children\n",
      "Topic 36: agreement talks negotiations two agreed end officials told proposal leaders\n",
      "Topic 37: st john chief james william president robert washington louis board\n",
      "Topic 38: commission waste local state garbage states public services town officials\n",
      "Topic 39: france europe countries european britain french british paris summit thatcher\n",
      "Topic 40: farmers farm agriculture tons million percent drought prices agricultural production\n",
      "Topic 41: air plane flight airlines airport pilots airline aircraft eastern planes\n",
      "Topic 42: committee members chairman house rep panel subcommittee congress lawmakers senate\n",
      "Topic 43: statement tuesday decision political called special made policy continue support\n",
      "Topic 44: film movie academy actor films entertainment disney actress story last\n",
      "Topic 45: iraq kuwait iraqi saudi iran gulf war arabia saddam persian\n",
      "Topic 46: space shuttle launch mission nasa earth venus spacecraft mars rocket\n",
      "Topic 47: tax income taxes benefits estate paid irs year returns federal\n",
      "Topic 48: claims safety accident trust victims accidents red payments miners people\n",
      "Topic 49: fire national cars service park county officials forest firefighters miles\n",
      "Topic 50: says american association say new business travel service companies written\n",
      "Topic 51: art museum home sold stolen collection sell sale works money\n",
      "Topic 52: department program service federal officials government agency office state law\n",
      "Topic 53: death murder victims police found crime shot died body victim\n",
      "Topic 54: city people housing mayor home homeless homes cities local residents\n",
      "Topic 55: bank billion banks debt first assets loan million bankruptcy savings\n",
      "Topic 56: dollar yen late gold london bid fell dollars ounce dealers\n",
      "Topic 57: people police government protest demonstrators protesters students front solidarity protests\n",
      "Topic 58: dukakis campaign jackson democratic republican presidential state convention primary governor\n",
      "Topic 59: million money year last fund pay funds paid years bills\n",
      "Topic 60: television news network cbs radio nbc tv broadcast abc show\n",
      "Topic 61: israel israeli palestinian peace arab jewish east palestinians plo occupied\n",
      "Topic 62: south north korea korean communist officials two remains war countries\n",
      "Topic 63: meeting told conference reporters monday made friday asked wednesday met\n",
      "Topic 64: school students university college student education schools board teachers teacher\n",
      "Topic 65: bush president reagan house white bushs fitzwater administration vice presidents\n",
      "Topic 66: united states american foreign state washington baker aid nations government\n",
      "Topic 67: west east german germany berlin germanys germans united kohl unification\n",
      "Topic 68: women club men members wine young aspirin society male female\n",
      "Topic 69: percent study poll survey found three population conducted released results\n",
      "Topic 70: children hospital care medical child parents ms doctors mother health\n",
      "Topic 71: police two man people authorities night bus car yearold taken\n",
      "Topic 72: news magazine book new newspaper editor wrote daily published times\n",
      "Topic 73: company inc offer corp stock co companys companies percent share\n",
      "Topic 74: aid nicaragua rebels contra contras government military sandinista war ortega\n",
      "Topic 75: bill senate house legislation sen congress measure committee vote rep\n",
      "Topic 76: plant environmental nuclear plants energy epa pollution industry chemical air\n",
      "Topic 77: group members national council groups years thursday two called member\n",
      "Topic 78: water state river department species fish wildlife turkey north endangered\n",
      "Topic 79: people years country america year last world free leave recent\n",
      "Topic 80: budget billion congress spending deficit cut administration cuts programs bush\n",
      "Topic 81: security bomb northern people british irish bombing killed ireland ira\n",
      "Topic 82: official news agency reported condition government officials today saying quoted\n",
      "Topic 83: percent year rate last increase month government report months rose\n",
      "Topic 84: aids health disease virus patients blood treatment heart drug cancer\n",
      "Topic 85: military defense force air pentagon army forces base navy soldiers\n",
      "Topic 86: game thompson won th first barry team two race second\n",
      "Topic 87: ship island coast navy boat sea ships two guard miles\n",
      "Topic 88: monday two day days tuesday last week three years weeks\n",
      "Topic 89: panama government noriega mexico human rights cuba philippines mexican cuban\n",
      "Topic 90: abortion ms humphrey smoking state law kennedy ban women souter\n",
      "Topic 91: cents cent lower futures higher prices bushel delivery pound exchange\n",
      "Topic 92: milken letter reed black two murphy government hunt alabama joseph\n",
      "Topic 93: years year two time million ago major large big number\n",
      "Topic 94: drug cocaine fbi drugs arrested agents agent federal customs authorities\n",
      "Topic 95: million year sales billion percent last share quarter earnings rose\n",
      "Topic 96: music year record award play band singer center awards musical\n",
      "Topic 97: north documents national case testimony reagan security walsh trial secret\n",
      "Topic 98: soviet union soviets gorbachev moscow treaty summit reagan nuclear arms\n",
      "Topic 99: people area miles damage ground two water earthquake quake state\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocabulary)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(u'Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
