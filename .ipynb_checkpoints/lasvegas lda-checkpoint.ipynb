{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAS VEGAS: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import LDA\n",
    "import lda\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import psi\n",
    "import collections\n",
    "import json\n",
    "from scipy import sparse\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data for Las vegas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Zipped_data_LV/lasvegas_word_adjective.json', 'r') as fp:\n",
    "    lv_wadj = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the vocabulary list with words present at least 10 times in all the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for i in lv_wadj.keys():\n",
    "    vocab.extend(lv_wadj[i][0][0])\n",
    "import collections\n",
    "lv_vocab10 = []\n",
    "d = collections.Counter(vocab)\n",
    "for w in vocab:\n",
    "    if d[w]>=10:\n",
    "        lv_vocab10.append(w)\n",
    "lv_vocab10 = list(set(lv_vocab10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10364"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lv_vocab10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('temp/lv_vocab10.npy',lv_vocab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionaries to map bid to an index and words to index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_word_to_index  = dict(zip(lv_vocab10,range(len(lv_vocab10))))\n",
    "lv_index_to_word = dict(zip(range(len(lv_vocab10)),lv_vocab10))\n",
    "lv_bid_to_index = dict(zip(lv_wadj.keys(),range(len(lv_wadj.keys()))))\n",
    "lv_index_to_bid = dict(zip(range(len(lv_wadj.keys())),lv_wadj.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dtm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM TRAIN AND TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n"
     ]
    }
   ],
   "source": [
    "reduced_lv = {}\n",
    "it = 0\n",
    "for (k,v) in lv_wadj.items():\n",
    "    if it % 50 == 0:\n",
    "        print it\n",
    "    new = []\n",
    "    for w in v[0][0]:\n",
    "        if w in lv_vocab10:\n",
    "            new.append(w)\n",
    "    reduced_lv[k] = new\n",
    "    it+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing some of the top 30 words most present in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toremove = ['food','place','service','time','restaurant','menu','order','price','dish','table','thing','person','experience','server','staff','way','bit','portion','lot','hour','star','plate','course','option','customer','liking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "nonzero_data = []\n",
    "rows_s = []\n",
    "cols_s = []\n",
    "\n",
    "for k in reduced_lv.keys():\n",
    "    counter = collections.Counter(reduced_lv[k])\n",
    "    for r in toremove:\n",
    "        counter[r] = 0\n",
    "    nonzero_data += list(np.floor(np.array(counter.values())*100/np.max(counter.values()))) #Rescaling to account for the important differences in the lengths of the reviews\n",
    "    rows_s += [lv_bid_to_index[k]]*len(counter.values())\n",
    "    cols_s += [lv_word_to_index[ck] for ck in counter.keys()]\n",
    "\n",
    "sparse_mat = sparse.csc_matrix((nonzero_data,(rows_s,cols_s)),shape = (len(lv_bid_to_index),len(lv_word_to_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the document term matrix for the complete data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lv_dtm = sparse_mat.toarray()\n",
    "np.save('temp/lv_dtm.npy',lv_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the data into training and validation/testing by splitting each document into 2 on an 80/20 basis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "for k in reduced_lv.keys():\n",
    "    spl = np.split(range(len(reduced_lv[k])),[int(np.floor(0.8*len(reduced_lv[k]))),len(reduced_lv[k])])\n",
    "    train[k] = np.array(reduced_lv[k])[spl[0]]\n",
    "    test[k] = np.array(reduced_lv[k])[spl[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "nonzero_data_tr = []\n",
    "rows_s_tr = []\n",
    "cols_s_tr = []\n",
    "nonzero_data_ts = []\n",
    "rows_s_ts = []\n",
    "cols_s_ts = []\n",
    "\n",
    "for k in reduced_lv.keys():\n",
    "    \n",
    "    counter_tr = collections.Counter(train[k])\n",
    "    for r in toremove:\n",
    "        counter_tr[r] = 0\n",
    "    nonzero_data_tr += counter_tr.values()\n",
    "    rows_s_tr += [lv_bid_to_index[k]]*len(counter_tr.values())\n",
    "    cols_s_tr += [lv_word_to_index[ck] for ck in counter_tr.keys()]\n",
    "    \n",
    "    counter_ts = collections.Counter(test[k])\n",
    "    for r in toremove:\n",
    "        counter_ts[r] = 0\n",
    "    nonzero_data_ts += counter_ts.values()\n",
    "    rows_s_ts += [lv_bid_to_index[k]]*len(counter_ts.values())\n",
    "    cols_s_ts += [lv_word_to_index[ck] for ck in counter_ts.keys()]\n",
    "\n",
    "sparse_mat_tr = sparse.csc_matrix((nonzero_data_tr,(rows_s_tr,cols_s_tr)),shape = (len(lv_bid_to_index),len(lv_word_to_index)))\n",
    "sparse_mat_ts = sparse.csc_matrix((nonzero_data_ts,(rows_s_ts,cols_s_ts)),shape = (len(lv_bid_to_index),len(lv_word_to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_lv_train = sparse_mat_tr.toarray()\n",
    "dtm_lv_test = sparse_mat_ts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('temp/dtm_lv_train.npy',dtm_lv_train)\n",
    "np.save('temp/dtm_lv_test.npy',dtm_lv_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the lda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rho(tau,kappa,t):\n",
    "\treturn pow(tau + t, - kappa)\n",
    "\n",
    "def digamma(mat):\n",
    "\tif (len(mat.shape) == 1):\n",
    "\t\treturn(psi(mat) - psi(np.sum(mat)))\n",
    "\telse:\n",
    "\t\treturn(psi(mat) - psi(np.sum(mat, 0))[np.newaxis,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_batch(dtm,ntopic,batch_size,tau,kappa):\n",
    "\tnvoc = dtm.shape[1]\n",
    "\tndoc = dtm.shape[0]\n",
    "\tnu = 1./ntopic\n",
    "\talpha = 1./ntopic\n",
    "\n",
    "\ttopics = np.random.gamma(100.,1./100.,(nvoc,ntopic))\n",
    "\tgamma  = np.random.gamma(100.,1./100.,(ndoc,ntopic))\n",
    "\n",
    "\tnumbatch = ndoc / batch_size\n",
    "\tbatches = np.array_split(range(ndoc),numbatch)\n",
    "\n",
    "\n",
    "\tfor it_batch in range(numbatch):\n",
    "\t\tELogBeta = digamma(topics)\n",
    "\t\tExpELogBeta = np.exp(ELogBeta)\n",
    "\t\t\n",
    "\t\ttemp_topics = np.zeros(topics.shape)\n",
    "\n",
    "\t\tindices = []\n",
    "\n",
    "\t\tfor d in batches[it_batch]:\n",
    "\t\t\t# print d\n",
    "\t\t\tids = np.nonzero(dtm[d,:])[0]\n",
    "\t\t\tindices.extend(ids)\n",
    "\t\t\tcts = dtm[d,ids]\n",
    "\t\t\tExpELogBetad = ExpELogBeta[ids,:]\n",
    "\n",
    "\t\t\tgammad = gamma[d,:]\n",
    "\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\n",
    "\t\t\t# print gammad\n",
    "\n",
    "\t\t\tfor inner_it in range(1000):\n",
    "\t\t\t\t\n",
    "\t\t\t\toldgammad = gammad\n",
    "\n",
    "\t\t\t\tphi =  ExpLogTethad * ExpELogBetad\n",
    "\t\t\t\tphi = phi / (phi.sum(axis=1)+0.00001)[:, np.newaxis]\n",
    "\n",
    "\t\t\t\tgammad = alpha + np.dot(cts,phi)\n",
    "\n",
    "\t\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\t\t\t\t# print gammad\n",
    "\n",
    "\t\t\t\tif np.mean((gammad-oldgammad)**2)<0.0000001:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t#print inner_it\n",
    "\t\t\tgamma[d,:] = gammad\n",
    "\n",
    "\t\t\ttemp_topics[ids,:] += phi * cts[:,np.newaxis]\n",
    "\n",
    "\t\tindices = np.unique(indices)\n",
    "\n",
    "\t\trt = rho(tau,kappa,it_batch)\n",
    "\n",
    "\t\ttopics[indices] = (1 - rt) * topics[indices,:] + rt * ndoc * (nu + temp_topics[indices,:]) / len(batches[it_batch])\n",
    "\n",
    "\treturn topics,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(lda,dtm,tau,kappa):\n",
    "\n",
    "\tntopic = lda[0].shape[1]\n",
    "\tnvoc = dtm.shape[1]\n",
    "\tndoc = dtm.shape[0]\n",
    "\tnu = 1./ntopic\n",
    "\talpha = 1./ntopic\n",
    "\n",
    "\ttopics = lda[0].copy()\n",
    "\tphi = np.random.gamma(100.,1./100.,(nvoc,ntopic))\n",
    "\tgamma  = np.random.gamma(100.,1./100.,(ndoc,ntopic))\n",
    "\n",
    "\tnumbatch = ndoc\n",
    "\tbatches = np.array_split(range(ndoc),numbatch)\n",
    "\n",
    "\tfor it_batch in range(numbatch):\n",
    "\t\tELogBeta = digamma(topics)\n",
    "\t\tExpELogBeta = np.exp(ELogBeta)\n",
    "\t\t\n",
    "\t\ttemp_topics = np.zeros(topics.shape)\n",
    "\n",
    "\t\tindices = []\n",
    "\n",
    "\t\tfor d in batches[it_batch]:\n",
    "\t\t\t# print d\n",
    "\t\t\tids = np.nonzero(dtm[d,:])[0]\n",
    "\t\t\tindices.extend(ids)\n",
    "\t\t\tcts = dtm[d,ids]\n",
    "\t\t\tExpELogBetad = ExpELogBeta[ids,:]\n",
    "\n",
    "\t\t\tgammad = gamma[d,:]\n",
    "\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\n",
    "\t\t\t# print gammad\n",
    "\n",
    "\t\t\tfor inner_it in range(1000):\n",
    "\t\t\t\t\n",
    "\t\t\t\toldgammad = gammad\n",
    "\n",
    "\t\t\t\tphi =  ExpLogTethad * ExpELogBetad\n",
    "\t\t\t\tphi = phi / (phi.sum(axis=1)+0.00001)[:, np.newaxis]\n",
    "\n",
    "\t\t\t\tgammad = alpha + np.dot(cts,phi)\n",
    "\n",
    "\t\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\t\t\t\t# print gammad\n",
    "\n",
    "\t\t\t\tif np.mean((gammad-oldgammad)**2)<0.0000001:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t#print inner_it\n",
    "\t\t\tgamma[d,:] = gammad\n",
    "\n",
    "\t\t\ttemp_topics[ids,:] += phi * cts[:,np.newaxis]\n",
    "\n",
    "\t\tindices = np.unique(indices)\n",
    "\t\tif indices.size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\trt = rho(tau,kappa,it_batch)\n",
    "\n",
    "\t\ttopics[indices] = (1 - rt) * topics[indices,:] + rt * ndoc * (nu + temp_topics[indices,:]) / len(batches[it_batch])\n",
    "\n",
    "\treturn topics,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(lda,newdocs,tau,kappa,perword = False):\n",
    "\t\n",
    "\tnew = inference(lda,newdocs,tau,kappa)\n",
    "\t\n",
    "\ttopics = new[0]\n",
    "\tgammas = new[1]\n",
    "\t\n",
    "\ttopics = topics/topics.sum(axis=0)\n",
    "\t\n",
    "\tif len(gammas.shape) == 1:\n",
    "\t\tgammas = gammas/np.sum(gammas)\n",
    "\t\tdoc_idx = np.nonzero(newdocs)[0]\n",
    "\t\tdoc_cts = newdocs[doc_idx]\n",
    "\t\treturn np.exp(-np.log(np.sum(np.dot(topics[doc_idx,:],gammas)*doc_cts))/np.sum(doc_cts))\n",
    "\t\n",
    "\telse:\n",
    "\t\tnorm = lambda x: x/np.sum(x)\n",
    "\t\tgammas = np.apply_along_axis(norm,axis = 1,arr = gammas)\n",
    "\t\t\n",
    "\t\tnum = 0\n",
    "\t\tdenom = 0\n",
    "\t\t\n",
    "\t\tfor i in range(gammas.shape[0]):\n",
    "\t\t\tdoc_idx = np.nonzero(newdocs[i,:])[0]\n",
    "\t\t\tdoc_cts = newdocs[i,doc_idx]\n",
    "\t\t\tnum = np.sum(np.log(np.dot(topics[doc_idx,:],gammas[i,:]))*doc_cts)\n",
    "\t\t\tdenom += np.sum(doc_cts)\n",
    "\t\t\t\n",
    "\t\tif ~perword:\n",
    "\t\t\treturn num\n",
    "\t\telse:\n",
    "\t\t\treturn num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perp(lda,newdocs,tau,kappa):\n",
    "    \n",
    "    from scipy.special import gammaln,psi\n",
    "    \n",
    "    new = inference(lda,newdocs,tau,kappa)\n",
    "    \n",
    "    score = 0\n",
    "    gamma = new[1]\n",
    "    topics = new[0]\n",
    "    Elogtheta = psi(gamma) - psi(np.sum(gamma, 1))[:,np.newaxis]\n",
    "    expElogtheta = np.exp(Elogtheta)\n",
    "    Elogbeta = digamma(topics)\n",
    "    # E[log p(docs | theta, beta)]\n",
    "    for d in range(0, newdocs.shape[0]):\n",
    "        gammad = gamma[d, :]\n",
    "        ids = np.nonzero(newdocs[d,:])[0]\n",
    "        cts = newdocs[d,ids]\n",
    "        phinorm = np.zeros(len(ids))\n",
    "        for i in range(0, len(ids)):\n",
    "            temp = Elogtheta[d, :] + Elogbeta[ids[i],:]\n",
    "            tmax = max(temp)\n",
    "            phinorm[i] = np.log(sum(np.exp(temp - tmax))) + tmax\n",
    "        score += np.sum(cts * phinorm)\n",
    "\n",
    "\n",
    "    # E[log p(theta | alpha) - log q(theta | gamma)]\n",
    "    score += np.sum((1./topics.shape[1] - gamma)*Elogtheta)\n",
    "    score += np.sum(gammaln(gamma) - gammaln(1./topics.shape[1]))\n",
    "    score += np.sum(gammaln(1/topics.shape[1]*topics.shape[1]) - gammaln(np.sum(gamma, 1)))\n",
    "\n",
    "#     Compensate for the subsampling of the population of documents\n",
    "    score = score * float(newdocs.shape[0]) / lda[0].shape[0]\n",
    "\n",
    "    # E[log p(beta | eta) - log q (beta | lambda)]\n",
    "    score = score + np.sum((1/topics.shape[1]-topics)*Elogbeta)\n",
    "    score = score + np.sum(gammaln(topics) - gammaln(1./topics.shape[1]))\n",
    "    score = score + np.sum(gammaln(1./topics.shape[1]*topics.shape[0]) - gammaln(np.sum(topics, 0)))\n",
    "\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 40\n",
    "kappa = 0.5\n",
    "tau = 1024\n",
    "\n",
    "K = range(30,90,10)\n",
    "\n",
    "perplexity_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 34s, sys: 13.6 s, total: 15min 47s\n",
      "Wall time: 15min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for k in K:\n",
    "    np.random.seed(0)\n",
    "    train_lda = lda_batch(dtm_lv_train,k,batchsize,tau,kappa)\n",
    "    perplexity_dict[k] = perplexity(train_lda,dtm_lv_test,tau,kappa,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_k = max(perplexity_dict, key=perplexity_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 s, sys: 253 ms, total: 39.2 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(0)\n",
    "model = lda_batch(lv_dtm,best_k,batchsize,tau,kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: rice chicken soup roll beef sushi sauce noodle lunch pork\n",
      "Topic 1: dog fry bun chili truck hotdog pretzel tot onion lemonade\n",
      "Topic 2: campus inconvenience popover thirst student contamination quenching ll yumm class\n",
      "Topic 3: bar drink beer night bartender waitress music friend game area\n",
      "Topic 4: change nugget ranch cheeseburger character bathroom hooker cop hiding teen\n",
      "Topic 5: tapas ceviche empanada paella plantain sangria date saltado sangrium la\n",
      "Topic 6: sandwich bread sub meat subway location airport cheese turkey deli\n",
      "Topic 7: store counter donut ass floor owner grocery wheat produce remark\n",
      "Topic 8: location drive employee minute manager window guy chicken cashier car\n",
      "Topic 9: discontent dippin tamal sympathy soooooo gesture printer transaction standpoint behalf\n",
      "Topic 10: popsicle chicharone cabana escort drink gelatin sirloin tasting stall cheek\n",
      "Topic 11: contamination cross bark inconvenience campus shuttle steak meat chicken piano\n",
      "Topic 12: attitude rice line kick local view music location pick theme\n",
      "Topic 13: cinnabon sisig shrimp crawfish crab juicy halo sauce fry tilapia\n",
      "Topic 14: theme street plastic finger school offering turkey mint tuna couple\n",
      "Topic 15: catfish fish puppy soul cornbread okra gumbo collard cat seafood\n",
      "Topic 16: drink taco ragoon beef night chicken lunch quesidilla rice beers\n",
      "Topic 17: steak dinner meal wine waiter lobster dessert salad appetizer night\n",
      "Topic 18: check street woman moment batter range vinaigrette salad charge sit\n",
      "Topic 19: correct straw discovery gps highway clientele lb wrapper heck jap\n",
      "Topic 20: breakfast egg pancake toast morning bacon hash coffee omelet waitress\n",
      "Topic 21: pupusa pupusas papusa chicharron curtido plantain poisoning revuelta salvadorean veronica\n",
      "Topic 22: taco salsa chicken bean burrito chip cheese sauce location meat\n",
      "Topic 23: wing boneless buffalo fry ghost ranch living drumstick pepper franchising\n",
      "Topic 24: sisig crawfish shrimp juicy sauce crab pajama seafood fry tilapia\n",
      "Topic 25: franchisee smh lau l&l shack twinky unlv flamingo nathan pb&j\n",
      "Topic 26: coffee bagel smoothie pastry chocolate cup ice tea gelato cream\n",
      "Topic 27: state booth management child rating issue compliment skin sister mind\n",
      "Topic 28: buffet selection dessert station line leg item variety crab quality\n",
      "Topic 29: lake pat stench production smoking boxing lakeside bar curve procedure\n",
      "Topic 30: chicken salad sauce burger meal cheese flavor fry lunch meat\n",
      "Topic 31: lechon halo pancit sisig adobo pata lumpia filipino sample kawali\n",
      "Topic 32: decorum sanwich subway location sandwich mail confusion employee daughter coupon\n",
      "Topic 33: pizza shuttle salad van wine waiter sauce meal dinner piano\n",
      "Topic 34: pizza rice parameter neighbourhood chicken lunch egg foo beef noodle\n",
      "Topic 35: bud piping patron yelp break ring smell bun cold perfection\n",
      "Topic 36: naan masala tikka paneer injera samosa tandoori samosas lassi chutney\n",
      "Topic 37: pizza crust cheese delivery slice sauce wing pepperoni garlic topping\n",
      "Topic 38: crisps sandwich co-worker guy couple bread fry pool size burger\n",
      "Topic 39: mintue nugget chicken hooker change cop salad judgement sandwich potato\n"
     ]
    }
   ],
   "source": [
    "topic_word = model[0].T  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(lv_vocab10)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(u'Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using KL divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.38629436,  1.38629436,  1.38629436,  1.38629436])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t= np.array([1,1,1,1])\n",
    "tt = np.array([0.25,0.25,0.25,0.25])\n",
    "t*np.log(t/tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kl_div_sym(v1,v2):\n",
    "    l = np.log(v1/v2)\n",
    "    return np.sum(v1*l) + np.sum(-v2*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = model[1]\n",
    "kl_mat = np.zeros((3822,3822))\n",
    "for i in range(3822):\n",
    "    for j in range(i+1,3822,1):\n",
    "        kl_mat[i,j] = kl_mat[j,i] = kl_div_sym(G[i,:]/np.sum(G[i,:]),G[j,:]/np.sum(G[j,:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
