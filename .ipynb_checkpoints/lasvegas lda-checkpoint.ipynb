{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAS VEGAS: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import LDA\n",
    "import lda\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import psi\n",
    "import collections\n",
    "import json\n",
    "from scipy import sparse\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data for Las vegas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Zipped_data_LV/lasvegas_word_adjective.json', 'r') as fp:\n",
    "    lv_wadj = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the vocabulary list with words present at least 10 times in all the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for i in lv_wadj.keys():\n",
    "    vocab.extend(lv_wadj[i][0][0])\n",
    "import collections\n",
    "lv_vocab10 = []\n",
    "d = collections.Counter(vocab)\n",
    "for w in vocab:\n",
    "    if d[w]>=10:\n",
    "        lv_vocab10.append(w)\n",
    "lv_vocab10 = list(set(lv_vocab10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10364"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lv_vocab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionaries to map bid to an index and words to index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_word_to_index  = dict(zip(lv_vocab10,range(len(lv_vocab10))))\n",
    "lv_index_to_word = dict(zip(range(len(lv_vocab10)),lv_vocab10))\n",
    "lv_bid_to_index = dict(zip(lv_wadj.keys(),range(len(lv_wadj.keys()))))\n",
    "lv_index_to_bid = dict(zip(range(len(lv_wadj.keys())),lv_wadj.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dtm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lv_dtm = np.load('../Zipped_data_LV/lasvegas_dtm.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3822, 10364)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM TRAIN AND TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n"
     ]
    }
   ],
   "source": [
    "reduced_lv = {}\n",
    "it = 0\n",
    "for (k,v) in lv_wadj.items():\n",
    "    if it % 50 == 0:\n",
    "        print it\n",
    "    new = []\n",
    "    for w in v[0][0]:\n",
    "        if w in lv_vocab10:\n",
    "            new.append(w)\n",
    "    reduced_lv[k] = new\n",
    "    it+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "for k in reduced_lv.keys():\n",
    "    spl = np.split(range(len(reduced_lv[k])),[int(np.floor(0.8*len(reduced_lv[k]))),len(reduced_lv[k])])\n",
    "    train[k] = np.array(reduced_lv[k])[spl[0]]\n",
    "    test[k] = np.array(reduced_lv[k])[spl[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "nonzero_data_tr = []\n",
    "rows_s_tr = []\n",
    "cols_s_tr = []\n",
    "nonzero_data_ts = []\n",
    "rows_s_ts = []\n",
    "cols_s_ts = []\n",
    "\n",
    "for k in reduced_lv.keys():\n",
    "    \n",
    "    counter_tr = collections.Counter(train[k])\n",
    "    nonzero_data_tr += counter_tr.values()\n",
    "    rows_s_tr += [lv_bid_to_index[k]]*len(counter_tr.values())\n",
    "    cols_s_tr += [lv_word_to_index[ck] for ck in counter_tr.keys()]\n",
    "    \n",
    "    counter_ts = collections.Counter(test[k])\n",
    "    nonzero_data_ts += counter_ts.values()\n",
    "    rows_s_ts += [lv_bid_to_index[k]]*len(counter_ts.values())\n",
    "    cols_s_ts += [lv_word_to_index[ck] for ck in counter_ts.keys()]\n",
    "\n",
    "sparse_mat_tr = sparse.csc_matrix((nonzero_data_tr,(rows_s_tr,cols_s_tr)),shape = (len(lv_bid_to_index),len(lv_word_to_index)))\n",
    "sparse_mat_ts = sparse.csc_matrix((nonzero_data_ts,(rows_s_ts,cols_s_ts)),shape = (len(lv_bid_to_index),len(lv_word_to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_lv_train = sparse_mat_tr.toarray()\n",
    "dtm_lv_test = sparse_mat_ts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the lda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rho(tau,kappa,t):\n",
    "\treturn pow(tau + t, - kappa)\n",
    "\n",
    "def digamma(mat):\n",
    "\tif (len(mat.shape) == 1):\n",
    "\t\treturn(psi(mat) - psi(np.sum(mat)))\n",
    "\telse:\n",
    "\t\treturn(psi(mat) - psi(np.sum(mat, 0))[np.newaxis,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_batch(dtm,ntopic,batch_size,tau,kappa):\n",
    "\tnvoc = dtm.shape[1]\n",
    "\tndoc = dtm.shape[0]\n",
    "\tnu = 1./ntopic\n",
    "\talpha = 1./ntopic\n",
    "\n",
    "\ttopics = np.random.gamma(100.,1./100.,(nvoc,ntopic))\n",
    "\tgamma  = np.random.gamma(100.,1./100.,(ndoc,ntopic))\n",
    "\n",
    "\tnumbatch = ndoc / batch_size\n",
    "\tbatches = np.array_split(range(ndoc),numbatch)\n",
    "\n",
    "\n",
    "\tfor it_batch in range(numbatch):\n",
    "\t\tELogBeta = digamma(topics)\n",
    "\t\tExpELogBeta = np.exp(ELogBeta)\n",
    "\t\t\n",
    "\t\ttemp_topics = np.zeros(topics.shape)\n",
    "\n",
    "\t\tindices = []\n",
    "\n",
    "\t\tfor d in batches[it_batch]:\n",
    "\t\t\t# print d\n",
    "\t\t\tids = np.nonzero(dtm[d,:])[0]\n",
    "\t\t\tindices.extend(ids)\n",
    "\t\t\tcts = dtm[d,ids]\n",
    "\t\t\tExpELogBetad = ExpELogBeta[ids,:]\n",
    "\n",
    "\t\t\tgammad = gamma[d,:]\n",
    "\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\n",
    "\t\t\t# print gammad\n",
    "\n",
    "\t\t\tfor inner_it in range(1000):\n",
    "\t\t\t\t\n",
    "\t\t\t\toldgammad = gammad\n",
    "\n",
    "\t\t\t\tphi =  ExpLogTethad * ExpELogBetad\n",
    "\t\t\t\tphi = phi / (phi.sum(axis=1)+0.00001)[:, np.newaxis]\n",
    "\n",
    "\t\t\t\tgammad = alpha + np.dot(cts,phi)\n",
    "\n",
    "\t\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\t\t\t\t# print gammad\n",
    "\n",
    "\t\t\t\tif np.mean((gammad-oldgammad)**2)<0.0000001:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t#print inner_it\n",
    "\t\t\tgamma[d,:] = gammad\n",
    "\n",
    "\t\t\ttemp_topics[ids,:] += phi * cts[:,np.newaxis]\n",
    "\n",
    "\t\tindices = np.unique(indices)\n",
    "\n",
    "\t\trt = rho(tau,kappa,it_batch)\n",
    "\n",
    "\t\ttopics[indices] = (1 - rt) * topics[indices,:] + rt * ndoc * (nu + temp_topics[indices,:]) / len(batches[it_batch])\n",
    "\n",
    "\treturn topics,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(lda,dtm,tau,kappa):\n",
    "\n",
    "\tntopic = lda[0].shape[1]\n",
    "\tnvoc = dtm.shape[1]\n",
    "\tndoc = dtm.shape[0]\n",
    "\tnu = 1./ntopic\n",
    "\talpha = 1./ntopic\n",
    "\n",
    "\ttopics = lda[0]\n",
    "\tphi = np.random.gamma(100.,1./100.,(nvoc,ntopic))\n",
    "\tgamma  = np.random.gamma(100.,1./100.,(ndoc,ntopic))\n",
    "\n",
    "\tnumbatch = ndoc\n",
    "\tbatches = np.array_split(range(ndoc),numbatch)\n",
    "\n",
    "\tfor i in range(1):\n",
    "\t\tfor it_batch in range(numbatch):\n",
    "\t\t\tELogBeta = digamma(topics)\n",
    "\t\t\tExpELogBeta = np.exp(ELogBeta)\n",
    "\t\t\t\n",
    "\t\t\ttemp_topics = np.zeros(topics.shape)\n",
    "\n",
    "\t\t\tindices = []\n",
    "\n",
    "\t\t\tfor d in batches[it_batch]:\n",
    "\t\t\t\t# print d\n",
    "\t\t\t\tids = np.nonzero(dtm[d,:])[0]\n",
    "\t\t\t\tindices.extend(ids)\n",
    "\t\t\t\tcts = dtm[d,ids]\n",
    "\t\t\t\tExpELogBetad = ExpELogBeta[ids,:]\n",
    "\n",
    "\t\t\t\tgammad = gamma[d,:]\n",
    "\t\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\n",
    "\t\t\t\t# print gammad\n",
    "\n",
    "\t\t\t\tfor inner_it in range(1000):\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\toldgammad = gammad\n",
    "\n",
    "\t\t\t\t\tphi =  ExpLogTethad * ExpELogBetad\n",
    "\t\t\t\t\tphi = phi / (phi.sum(axis=1)+0.00001)[:, np.newaxis]\n",
    "\n",
    "\t\t\t\t\tgammad = alpha + np.dot(cts,phi)\n",
    "\n",
    "\t\t\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\t\t\t\t\t# print gammad\n",
    "\n",
    "\t\t\t\t\tif np.mean((gammad-oldgammad)**2)<0.0000001:\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t# print inner_it\n",
    "\t\t\t\tgamma[d,:] = gammad\n",
    "\n",
    "\t\t\t\ttemp_topics[ids,:] += phi * cts[:,np.newaxis]\n",
    "\n",
    "\t\t\tindices = np.unique(indices)\n",
    "\n",
    "\t\t\trt = rho(tau,kappa,it_batch)\n",
    "\n",
    "\t\t\ttopics[indices] = (1 - rt) * topics[indices,:] + rt * ndoc * (nu + temp_topics[indices,:]) / len(batches[it_batch])\n",
    "\n",
    "\treturn topics,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(lda,newdocs,tau,kappa,perword = False):\n",
    "\t\n",
    "\tnew = inference(lda,newdocs,tau,kappa)\n",
    "\t\n",
    "\ttopics = new[0]\n",
    "\tgammas = new[1]\n",
    "\t\n",
    "\ttopics = topics/topics.sum(axis=0)\n",
    "\t\n",
    "\tif len(gammas.shape) == 1:\n",
    "\t\tgammas = gammas/np.sum(gammas)\n",
    "\t\tdoc_idx = np.nonzero(newdocs)[0]\n",
    "\t\tdoc_cts = newdocs[doc_idx]\n",
    "\t\treturn np.exp(-np.log(np.sum(np.dot(topics[doc_idx,:],gammas)*doc_cts))/np.sum(doc_cts))\n",
    "\t\n",
    "\telse:\n",
    "\t\tnorm = lambda x: x/np.sum(x)\n",
    "\t\tgammas = np.apply_along_axis(norm,axis = 1,arr = gammas)\n",
    "\t\t\n",
    "\t\tnum = 0\n",
    "\t\tdenom = 0\n",
    "\t\t\n",
    "\t\tfor i in range(gammas.shape[0]):\n",
    "\t\t\tdoc_idx = np.nonzero(newdocs[i,:])[0]\n",
    "\t\t\tdoc_cts = newdocs[i,doc_idx]\n",
    "\t\t\tnum = np.sum(np.log(np.dot(topics[doc_idx,:],gammas[i,:]))*doc_cts)\n",
    "\t\t\tdenom += np.sum(doc_cts)\n",
    "\t\t\t\n",
    "\t\tif ~perword:\n",
    "\t\t\treturn num\n",
    "\t\telse:\n",
    "\t\t\treturn num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 30\n",
    "kappa = 0.5\n",
    "tau = 1024\n",
    "\n",
    "K = range(30,80,10)\n",
    "\n",
    "perplexity_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 51s, sys: 10.3 s, total: 12min 1s\n",
      "Wall time: 12min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for k in K:\n",
    "    np.random.seed(0)\n",
    "    train_lda = lda_batch(dtm_lv_train,k,batchsize,tau,kappa)\n",
    "    perplexity_dict[k] = perplexity(train_lda,dtm_lv_test,tau,kappa,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_k = max(perplexity_dict, key=perplexity_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 2.5 s, total: 1min 34s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(0)\n",
    "model = lda_batch(lv_dtm,best_k,batchsize,tau,kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: consist membership actuality southwest pluss malt pil softball powder quacamole\n",
      "Topic 1: pupusa win-win heffer dawn hurdle puzzle bellman glamour heath splurge\n",
      "Topic 2: spacing mackarel cutlery fu green garage stomache propane mess maitake\n",
      "Topic 3: document pisco gazpacho half-roll godzilla length engagement paymon feat collar\n",
      "Topic 4: happiness orange carry grimaldis pop overeating wood-fire hipster meat duper\n",
      "Topic 5: basketball changer skanky goat arancine epitome methink skepticism habitat gelatos\n",
      "Topic 6: hurdle mackarel overload grate variant lark education wort ashtray use\n",
      "Topic 7: deed sourdough coffeehouse pisco document bundle stability dollar gelee ta\n",
      "Topic 8: overload motorcycle jap meatless tarter cavier chip reordering decision tab\n",
      "Topic 9: splurge unison flagship poet trashy upside surrounding ass demi-glaze importance\n",
      "Topic 10: break pisco saddest gang document date trudge pin suicide stability\n",
      "Topic 11: goody becasue hungover thickness mullet offender taziki dinning zin bartenders\n",
      "Topic 12: sammie swoop tzatziki gem veranda cell tendon tad parmesean soufflé\n",
      "Topic 13: comprehension haste assurance touchscreen enthusiast blanco re-fill streusel lint card\n",
      "Topic 14: document pisco tinga frenzy tobiko sprout parsnip gun engagement effing\n",
      "Topic 15: lindo wheel engagement injustice moo veteran pisco frenzy office document\n",
      "Topic 16: quota orange asian-fusion toffee cheese cardiologist martini cheapness cask pasilla\n",
      "Topic 17: pisco document tinga engagement openness purity sprout gun douchebag photography\n",
      "Topic 18: curling restauraunt godzilla veg chuckle cabinet slushy blanco wellington reggae\n",
      "Topic 19: carpacio unicorn trooper poulet pang need receive chau genoa rick\n",
      "Topic 20: break gang saddest date pisco trudge suicide pin furnishing document\n",
      "Topic 21: pager document lapse collar yayo veteran loathing engagement oak pisco\n",
      "Topic 22: coolness buena beefsteak choi e-mail sort ba monorail ko veg\n",
      "Topic 23: collar marination béarnaise stratosphere poppyseed strip-mall selecting cole crossaint siew\n",
      "Topic 24: break gang saddest date pisco furnishing pin strip-mall sud solo\n",
      "Topic 25: grouper platform hippie poppyseed tarama phoenix prefect maraschino marination mahi\n",
      "Topic 26: buena carton selecting yellow-tail papusa booklet bloodys insect ye poet\n",
      "Topic 27: stability coffeehouse bulgolgi parchment doe stress allowance deed bfs resaurant\n",
      "Topic 28: gunk bin adult schon loaf yayo bloodys realization koma insalata\n",
      "Topic 29: xxx selecting analysis pisco quantity pucker document close-by openness paymon\n",
      "Topic 30: pisco document godzilla selecting analysis openness paymon xxx drinks feat\n",
      "Topic 31: wort healthier non-smoker bin nazi sesame complimentary county glamour chord\n",
      "Topic 32: hall increase grabber chopping lag muffuletta sum sua protection immense\n",
      "Topic 33: facsimile convert mintue accountability dab office softball sister input brat\n",
      "Topic 34: mind crabcake decision commotion dryness decadence seat tendon decorator toffee\n",
      "Topic 35: vinegar valentine clumpy poppyseed calculator comprehension zin document doo posted\n",
      "Topic 36: cola calculator wizard local ba non-smoker protection donburi chuckle breading\n",
      "Topic 37: life visitor pluss supreme chill garage inexperience wood-fire manga kawali\n",
      "Topic 38: document gazpacho pisco xxx selecting neglect sever frenzy office quantity\n",
      "Topic 39: sichuan proximity blast cole timing stilt drinks prank concierge yellow-tail\n",
      "Topic 40: dinky drain sud half-roll feat trend document ratatouille godzilla vienna\n",
      "Topic 41: document bagoong frenzy office engagement pisco paymon godzilla feat rue\n",
      "Topic 42: accomplishment setback slope siew cutlery like charleston sourness fresh-tasting ying\n",
      "Topic 43: stein document pisco 's openness paymon engagement frenzy siew cabinet\n",
      "Topic 44: pp break softball frenzy pisco curtain document carry date chopping\n",
      "Topic 45: softball veteran rue break pisco gent aji hurdle hrs vomiting\n",
      "Topic 46: chateaubriand gateau cruncy calculator supreme torn tapioca hock supervision buffett\n",
      "Topic 47: etiquette say seasonal foie maraschino grate guest mayo manapua comfort\n",
      "Topic 48: carpacio taco zin framboise grubbin musashi dad tourism toffee wind\n",
      "Topic 49: music gateau homework samosa freak county receive yellow-tail fizz guinness\n",
      "Topic 50: taco document sprout purity vision calling puree pisco gun faint\n",
      "Topic 51: equivalent manapua final maitake sichuan cowgirl bloodys cristo placing lagoon\n",
      "Topic 52: surcharge stratosphere cha jaunt rescue opinon win-win sidebar slow nori\n",
      "Topic 53: embarrassment luau guitar expected chip spiciness cole-slaw dawn infestation foodie\n",
      "Topic 54: pico hungover coca swoop demon topper cristo circus tobiko tendon\n",
      "Topic 55: pager document tangerine lapse yayo pitum posted slope bacony scheme\n",
      "Topic 56: guitar comb book sinner mahi dusk beaucoup kimono euro recovery\n",
      "Topic 57: stripsteak core swoop disease favorite bin director ravioli music disservice\n",
      "Topic 58: document pisco engagement frenzy openness pluss siew collar feat office\n",
      "Topic 59: nazi frap mac'n'cheese surrounding pickup collar testing hardest director boa\n",
      "Topic 60: hooray outskirt pop overload ying slope blanco restarant cookware ragoon\n",
      "Topic 61: suon dinning receive tum scallion chip reluctance kickass buena splurge\n",
      "Topic 62: wi-fi nathan footprint detector chocolate touchscreen local photography input wood-fire\n",
      "Topic 63: siren chip hunt folk torte chrome breadth xtreme cast-iron rosemary\n",
      "Topic 64: lag multitude hipster crossaint towelette pisco roma wind office concierge\n",
      "Topic 65: builder bellman openness spun document puzzle control pisco paymon engagement\n",
      "Topic 66: input maraschino gruyere youngin envision taziki exhibit promotion skirt cooking\n",
      "Topic 67: break pisco date strip-mall swoop frenzy document tinga sud puree\n",
      "Topic 68: document pager collar oak veteran pisco roaster yayo teammate slope\n",
      "Topic 69: openness pepe decadence freak wellington sichuan downtown paymon input wear\n"
     ]
    }
   ],
   "source": [
    "topic_word = model[0].T  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(lv_vocab10)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(u'Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
