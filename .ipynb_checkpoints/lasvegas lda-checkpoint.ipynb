{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAS VEGAS: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import LDA\n",
    "import lda\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import psi\n",
    "import collections\n",
    "import json\n",
    "from scipy import sparse\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data for Las vegas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Zipped_data_LV/lasvegas_word_adjective.json', 'r') as fp:\n",
    "    lv_wadj = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the vocabulary list with words present at least 10 times in all the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for i in lv_wadj.keys():\n",
    "    vocab.extend(lv_wadj[i][0][0])\n",
    "import collections\n",
    "lv_vocab10 = []\n",
    "d = collections.Counter(vocab)\n",
    "for w in vocab:\n",
    "    if d[w]>=10:\n",
    "        lv_vocab10.append(w)\n",
    "lv_vocab10 = list(set(lv_vocab10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10364"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lv_vocab10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('temp/lv_vocab10.npy',lv_vocab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionaries to map bid to an index and words to index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_word_to_index  = dict(zip(lv_vocab10,range(len(lv_vocab10))))\n",
    "lv_index_to_word = dict(zip(range(len(lv_vocab10)),lv_vocab10))\n",
    "lv_bid_to_index = dict(zip(lv_wadj.keys(),range(len(lv_wadj.keys()))))\n",
    "lv_index_to_bid = dict(zip(range(len(lv_wadj.keys())),lv_wadj.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dtm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM TRAIN AND TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n"
     ]
    }
   ],
   "source": [
    "reduced_lv = {}\n",
    "it = 0\n",
    "for (k,v) in lv_wadj.items():\n",
    "    if it % 50 == 0:\n",
    "        print it\n",
    "    new = []\n",
    "    for w in v[0][0]:\n",
    "        if w in lv_vocab10:\n",
    "            new.append(w)\n",
    "    reduced_lv[k] = new\n",
    "    it+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing some of the top 30 words most present in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toremove = ['food','place','service','time','restaurant','menu','order','price','dish','table','thing','person','experience','server','staff','way','bit','portion','lot','hour','star','plate','course','option','customer','liking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "nonzero_data = []\n",
    "rows_s = []\n",
    "cols_s = []\n",
    "\n",
    "for k in reduced_lv.keys():\n",
    "    counter = collections.Counter(reduced_lv[k])\n",
    "    for r in toremove:\n",
    "        counter[r] = 0\n",
    "    nonzero_data += list(np.floor(np.array(counter.values())*100/np.max(counter.values()))) #Rescaling to account for the important differences in the lengths of the reviews\n",
    "    rows_s += [lv_bid_to_index[k]]*len(counter.values())\n",
    "    cols_s += [lv_word_to_index[ck] for ck in counter.keys()]\n",
    "\n",
    "sparse_mat = sparse.csc_matrix((nonzero_data,(rows_s,cols_s)),shape = (len(lv_bid_to_index),len(lv_word_to_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the document term matrix for the complete data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lv_dtm = sparse_mat.toarray()\n",
    "np.save('temp/lv_dtm.npy',lv_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the data into training and validation/testing by splitting each document into 2 on an 80/20 basis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "for k in reduced_lv.keys():\n",
    "    spl = np.split(range(len(reduced_lv[k])),[int(np.floor(0.8*len(reduced_lv[k]))),len(reduced_lv[k])])\n",
    "    train[k] = np.array(reduced_lv[k])[spl[0]]\n",
    "    test[k] = np.array(reduced_lv[k])[spl[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "nonzero_data_tr = []\n",
    "rows_s_tr = []\n",
    "cols_s_tr = []\n",
    "nonzero_data_ts = []\n",
    "rows_s_ts = []\n",
    "cols_s_ts = []\n",
    "\n",
    "for k in reduced_lv.keys():\n",
    "    \n",
    "    counter_tr = collections.Counter(train[k])\n",
    "    for r in toremove:\n",
    "        counter_tr[r] = 0\n",
    "    nonzero_data_tr += counter_tr.values()\n",
    "    rows_s_tr += [lv_bid_to_index[k]]*len(counter_tr.values())\n",
    "    cols_s_tr += [lv_word_to_index[ck] for ck in counter_tr.keys()]\n",
    "    \n",
    "    counter_ts = collections.Counter(test[k])\n",
    "    for r in toremove:\n",
    "        counter_ts[r] = 0\n",
    "    nonzero_data_ts += counter_ts.values()\n",
    "    rows_s_ts += [lv_bid_to_index[k]]*len(counter_ts.values())\n",
    "    cols_s_ts += [lv_word_to_index[ck] for ck in counter_ts.keys()]\n",
    "\n",
    "sparse_mat_tr = sparse.csc_matrix((nonzero_data_tr,(rows_s_tr,cols_s_tr)),shape = (len(lv_bid_to_index),len(lv_word_to_index)))\n",
    "sparse_mat_ts = sparse.csc_matrix((nonzero_data_ts,(rows_s_ts,cols_s_ts)),shape = (len(lv_bid_to_index),len(lv_word_to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_lv_train = sparse_mat_tr.toarray()\n",
    "dtm_lv_test = sparse_mat_ts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('temp/dtm_lv_train.npy',dtm_lv_train)\n",
    "np.save('temp/dtm_lv_test.npy',dtm_lv_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the lda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rho(tau,kappa,t):\n",
    "\treturn pow(tau + t, - kappa)\n",
    "\n",
    "def digamma(mat):\n",
    "\tif (len(mat.shape) == 1):\n",
    "\t\treturn(psi(mat) - psi(np.sum(mat)))\n",
    "\telse:\n",
    "\t\treturn(psi(mat) - psi(np.sum(mat, 0))[np.newaxis,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_batch(dtm,ntopic,batch_size,tau,kappa):\n",
    "\tnvoc = dtm.shape[1]\n",
    "\tndoc = dtm.shape[0]\n",
    "\tnu = 1./ntopic\n",
    "\talpha = 1./ntopic\n",
    "\n",
    "\ttopics = np.random.gamma(100.,1./100.,(nvoc,ntopic))\n",
    "\tgamma  = np.random.gamma(100.,1./100.,(ndoc,ntopic))\n",
    "\n",
    "\tnumbatch = ndoc / batch_size\n",
    "\tbatches = np.array_split(range(ndoc),numbatch)\n",
    "\n",
    "\n",
    "\tfor it_batch in range(numbatch):\n",
    "\t\tELogBeta = digamma(topics)\n",
    "\t\tExpELogBeta = np.exp(ELogBeta)\n",
    "\t\t\n",
    "\t\ttemp_topics = np.zeros(topics.shape)\n",
    "\n",
    "\t\tindices = []\n",
    "\n",
    "\t\tfor d in batches[it_batch]:\n",
    "\t\t\t# print d\n",
    "\t\t\tids = np.nonzero(dtm[d,:])[0]\n",
    "\t\t\tindices.extend(ids)\n",
    "\t\t\tcts = dtm[d,ids]\n",
    "\t\t\tExpELogBetad = ExpELogBeta[ids,:]\n",
    "\n",
    "\t\t\tgammad = gamma[d,:]\n",
    "\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\n",
    "\t\t\t# print gammad\n",
    "\n",
    "\t\t\tfor inner_it in range(1000):\n",
    "\t\t\t\t\n",
    "\t\t\t\toldgammad = gammad\n",
    "\n",
    "\t\t\t\tphi =  ExpLogTethad * ExpELogBetad\n",
    "\t\t\t\tphi = phi / (phi.sum(axis=1)+0.00001)[:, np.newaxis]\n",
    "\n",
    "\t\t\t\tgammad = alpha + np.dot(cts,phi)\n",
    "\n",
    "\t\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\t\t\t\t# print gammad\n",
    "\n",
    "\t\t\t\tif np.mean((gammad-oldgammad)**2)<0.0000001:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t#print inner_it\n",
    "\t\t\tgamma[d,:] = gammad\n",
    "\n",
    "\t\t\ttemp_topics[ids,:] += phi * cts[:,np.newaxis]\n",
    "\n",
    "\t\tindices = np.unique(indices)\n",
    "\n",
    "\t\trt = rho(tau,kappa,it_batch)\n",
    "\n",
    "\t\ttopics[indices] = (1 - rt) * topics[indices,:] + rt * ndoc * (nu + temp_topics[indices,:]) / len(batches[it_batch])\n",
    "\n",
    "\treturn topics,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(lda,dtm,tau,kappa):\n",
    "\n",
    "\tntopic = lda[0].shape[1]\n",
    "\tnvoc = dtm.shape[1]\n",
    "\tndoc = dtm.shape[0]\n",
    "\tnu = 1./ntopic\n",
    "\talpha = 1./ntopic\n",
    "\n",
    "\ttopics = lda[0].copy()\n",
    "\tphi = np.random.gamma(100.,1./100.,(nvoc,ntopic))\n",
    "\tgamma  = np.random.gamma(100.,1./100.,(ndoc,ntopic))\n",
    "\n",
    "\tnumbatch = ndoc\n",
    "\tbatches = np.array_split(range(ndoc),numbatch)\n",
    "\n",
    "\tfor it_batch in range(numbatch):\n",
    "\t\tELogBeta = digamma(topics)\n",
    "\t\tExpELogBeta = np.exp(ELogBeta)\n",
    "\t\t\n",
    "\t\ttemp_topics = np.zeros(topics.shape)\n",
    "\n",
    "\t\tindices = []\n",
    "\n",
    "\t\tfor d in batches[it_batch]:\n",
    "\t\t\t# print d\n",
    "\t\t\tids = np.nonzero(dtm[d,:])[0]\n",
    "\t\t\tindices.extend(ids)\n",
    "\t\t\tcts = dtm[d,ids]\n",
    "\t\t\tExpELogBetad = ExpELogBeta[ids,:]\n",
    "\n",
    "\t\t\tgammad = gamma[d,:]\n",
    "\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\n",
    "\t\t\t# print gammad\n",
    "\n",
    "\t\t\tfor inner_it in range(1000):\n",
    "\t\t\t\t\n",
    "\t\t\t\toldgammad = gammad\n",
    "\n",
    "\t\t\t\tphi =  ExpLogTethad * ExpELogBetad\n",
    "\t\t\t\tphi = phi / (phi.sum(axis=1)+0.00001)[:, np.newaxis]\n",
    "\n",
    "\t\t\t\tgammad = alpha + np.dot(cts,phi)\n",
    "\n",
    "\t\t\t\tElogTethad = digamma(gammad)\n",
    "\t\t\t\tExpLogTethad = np.exp(ElogTethad)\n",
    "\t\t\t\t# print gammad\n",
    "\n",
    "\t\t\t\tif np.mean((gammad-oldgammad)**2)<0.0000001:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t#print inner_it\n",
    "\t\t\tgamma[d,:] = gammad\n",
    "\n",
    "\t\t\ttemp_topics[ids,:] += phi * cts[:,np.newaxis]\n",
    "\n",
    "\t\tindices = np.unique(indices)\n",
    "\t\tif indices.size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\trt = rho(tau,kappa,it_batch)\n",
    "\n",
    "\t\ttopics[indices] = (1 - rt) * topics[indices,:] + rt * ndoc * (nu + temp_topics[indices,:]) / len(batches[it_batch])\n",
    "\n",
    "\treturn topics,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(lda,newdocs,tau,kappa,perword = False):\n",
    "\t\n",
    "\tnew = inference(lda,newdocs,tau,kappa)\n",
    "\t\n",
    "\ttopics = new[0]\n",
    "\tgammas = new[1]\n",
    "\t\n",
    "\ttopics = topics/topics.sum(axis=0)\n",
    "\t\n",
    "\tif len(gammas.shape) == 1:\n",
    "\t\tgammas = gammas/np.sum(gammas)\n",
    "\t\tdoc_idx = np.nonzero(newdocs)[0]\n",
    "\t\tdoc_cts = newdocs[doc_idx]\n",
    "\t\treturn np.exp(-np.log(np.sum(np.dot(topics[doc_idx,:],gammas)*doc_cts))/np.sum(doc_cts))\n",
    "\t\n",
    "\telse:\n",
    "\t\tnorm = lambda x: x/np.sum(x)\n",
    "\t\tgammas = np.apply_along_axis(norm,axis = 1,arr = gammas)\n",
    "\t\t\n",
    "\t\tnum = 0\n",
    "\t\tdenom = 0\n",
    "\t\t\n",
    "\t\tfor i in range(gammas.shape[0]):\n",
    "\t\t\tdoc_idx = np.nonzero(newdocs[i,:])[0]\n",
    "\t\t\tdoc_cts = newdocs[i,doc_idx]\n",
    "\t\t\tnum = np.sum(np.log(np.dot(topics[doc_idx,:],gammas[i,:]))*doc_cts)\n",
    "\t\t\tdenom += np.sum(doc_cts)\n",
    "\t\t\t\n",
    "\t\tif ~perword:\n",
    "\t\t\treturn num\n",
    "\t\telse:\n",
    "\t\t\treturn num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perp(lda,newdocs,tau,kappa):\n",
    "    \n",
    "    from scipy.special import gammaln,psi\n",
    "    \n",
    "    new = inference(lda,newdocs,tau,kappa)\n",
    "    \n",
    "    score = 0\n",
    "    gamma = new[1]\n",
    "    topics = new[0]\n",
    "    Elogtheta = psi(gamma) - psi(np.sum(gamma, 1))[:,np.newaxis]\n",
    "    expElogtheta = np.exp(Elogtheta)\n",
    "    Elogbeta = digamma(topics)\n",
    "    # E[log p(docs | theta, beta)]\n",
    "    for d in range(0, newdocs.shape[0]):\n",
    "        gammad = gamma[d, :]\n",
    "        ids = np.nonzero(newdocs[d,:])[0]\n",
    "        cts = newdocs[d,ids]\n",
    "        phinorm = np.zeros(len(ids))\n",
    "        for i in range(0, len(ids)):\n",
    "            temp = Elogtheta[d, :] + Elogbeta[ids[i],:]\n",
    "            tmax = max(temp)\n",
    "            phinorm[i] = np.log(sum(np.exp(temp - tmax))) + tmax\n",
    "        score += np.sum(cts * phinorm)\n",
    "\n",
    "\n",
    "    # E[log p(theta | alpha) - log q(theta | gamma)]\n",
    "    score += np.sum((1./topics.shape[1] - gamma)*Elogtheta)\n",
    "    score += np.sum(gammaln(gamma) - gammaln(1./topics.shape[1]))\n",
    "    score += np.sum(gammaln(1/topics.shape[1]*topics.shape[1]) - gammaln(np.sum(gamma, 1)))\n",
    "\n",
    "#     Compensate for the subsampling of the population of documents\n",
    "    score = score * float(newdocs.shape[0]) / lda[0].shape[0]\n",
    "\n",
    "    # E[log p(beta | eta) - log q (beta | lambda)]\n",
    "    score = score + np.sum((1/topics.shape[1]-topics)*Elogbeta)\n",
    "    score = score + np.sum(gammaln(topics) - gammaln(1./topics.shape[1]))\n",
    "    score = score + np.sum(gammaln(1./topics.shape[1]*topics.shape[0]) - gammaln(np.sum(topics, 0)))\n",
    "\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 40\n",
    "kappa = 0.5\n",
    "tau = 1024\n",
    "\n",
    "K = range(30,90,10)\n",
    "\n",
    "perplexity_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 34s, sys: 13.6 s, total: 15min 47s\n",
      "Wall time: 15min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for k in K:\n",
    "    np.random.seed(0)\n",
    "    train_lda = lda_batch(dtm_lv_train,k,batchsize,tau,kappa)\n",
    "    perplexity_dict[k] = perplexity(train_lda,dtm_lv_test,tau,kappa,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_k = max(perplexity_dict, key=perplexity_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 s, sys: 253 ms, total: 39.2 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(0)\n",
    "model = lda_batch(lv_dtm,best_k,batchsize,tau,kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: rice chicken soup roll beef sushi sauce noodle lunch pork\n",
      "Topic 1: dog fry bun chili truck hotdog pretzel tot onion lemonade\n",
      "Topic 2: campus inconvenience popover thirst student contamination quenching ll yumm class\n",
      "Topic 3: bar drink beer night bartender waitress music friend game area\n",
      "Topic 4: change nugget ranch cheeseburger character bathroom hooker cop hiding teen\n",
      "Topic 5: tapas ceviche empanada paella plantain sangria date saltado sangrium la\n",
      "Topic 6: sandwich bread sub meat subway location airport cheese turkey deli\n",
      "Topic 7: store counter donut ass floor owner grocery wheat produce remark\n",
      "Topic 8: location drive employee minute manager window guy chicken cashier car\n",
      "Topic 9: discontent dippin tamal sympathy soooooo gesture printer transaction standpoint behalf\n",
      "Topic 10: popsicle chicharone cabana escort drink gelatin sirloin tasting stall cheek\n",
      "Topic 11: contamination cross bark inconvenience campus shuttle steak meat chicken piano\n",
      "Topic 12: attitude rice line kick local view music location pick theme\n",
      "Topic 13: cinnabon sisig shrimp crawfish crab juicy halo sauce fry tilapia\n",
      "Topic 14: theme street plastic finger school offering turkey mint tuna couple\n",
      "Topic 15: catfish fish puppy soul cornbread okra gumbo collard cat seafood\n",
      "Topic 16: drink taco ragoon beef night chicken lunch quesidilla rice beers\n",
      "Topic 17: steak dinner meal wine waiter lobster dessert salad appetizer night\n",
      "Topic 18: check street woman moment batter range vinaigrette salad charge sit\n",
      "Topic 19: correct straw discovery gps highway clientele lb wrapper heck jap\n",
      "Topic 20: breakfast egg pancake toast morning bacon hash coffee omelet waitress\n",
      "Topic 21: pupusa pupusas papusa chicharron curtido plantain poisoning revuelta salvadorean veronica\n",
      "Topic 22: taco salsa chicken bean burrito chip cheese sauce location meat\n",
      "Topic 23: wing boneless buffalo fry ghost ranch living drumstick pepper franchising\n",
      "Topic 24: sisig crawfish shrimp juicy sauce crab pajama seafood fry tilapia\n",
      "Topic 25: franchisee smh lau l&l shack twinky unlv flamingo nathan pb&j\n",
      "Topic 26: coffee bagel smoothie pastry chocolate cup ice tea gelato cream\n",
      "Topic 27: state booth management child rating issue compliment skin sister mind\n",
      "Topic 28: buffet selection dessert station line leg item variety crab quality\n",
      "Topic 29: lake pat stench production smoking boxing lakeside bar curve procedure\n",
      "Topic 30: chicken salad sauce burger meal cheese flavor fry lunch meat\n",
      "Topic 31: lechon halo pancit sisig adobo pata lumpia filipino sample kawali\n",
      "Topic 32: decorum sanwich subway location sandwich mail confusion employee daughter coupon\n",
      "Topic 33: pizza shuttle salad van wine waiter sauce meal dinner piano\n",
      "Topic 34: pizza rice parameter neighbourhood chicken lunch egg foo beef noodle\n",
      "Topic 35: bud piping patron yelp break ring smell bun cold perfection\n",
      "Topic 36: naan masala tikka paneer injera samosa tandoori samosas lassi chutney\n",
      "Topic 37: pizza crust cheese delivery slice sauce wing pepperoni garlic topping\n",
      "Topic 38: crisps sandwich co-worker guy couple bread fry pool size burger\n",
      "Topic 39: mintue nugget chicken hooker change cop salad judgement sandwich potato\n"
     ]
    }
   ],
   "source": [
    "topic_word = model[0].T  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(lv_vocab10)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(u'Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('temp/topics_lv.npy',model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('temp/assignment_lv.npy',model[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to find the best hyperparametrisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 6s, sys: 11.9 s, total: 18min 18s\n",
      "Wall time: 18min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Kappa = [0.5,0.6,0.7]\n",
    "Tau = [216,512,1024]\n",
    "perplexity_dict2 = {}\n",
    "\n",
    "for kappa in Kappa:\n",
    "    for tau in Tau:\n",
    "        np.random.seed(0)\n",
    "        train_lda = lda_batch(dtm_lv_train,best_k,batchsize,tau,kappa)\n",
    "        perplexity_dict2[(tau,kappa)] = perplexity(train_lda,dtm_lv_test,tau,kappa,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 0.7)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_combi = max(perplexity_dict2, key=perplexity_dict2.get)\n",
    "best_combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.8 s, sys: 742 ms, total: 35.5 s\n",
      "Wall time: 36.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(0)\n",
    "model_bis = lda_batch(lv_dtm,best_k,batchsize,best_combi[0],best_combi[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('temp/topics2_lv.npy',model_bis[0])\n",
    "np.save('temp/assignment2_lv.npy',model_bis[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: rice chicken soup beef roll lunch sauce sushi noodle pork\n",
      "Topic 1: dog hotdog chili pretzel bun fry sauerkraut sausage mustard corndog\n",
      "Topic 2: popover steak lobster meal flavor dinner cheese salad potato meat\n",
      "Topic 3: bar drink beer bartender night game music wing waitress pool\n",
      "Topic 4: nugget change discovery hooker salad cop hiding chicken soup dressing\n",
      "Topic 5: date tapas cheese sangria sauce sangrium pitcher drink paella strip\n",
      "Topic 6: sandwich bread sub subway turkey meat location deli employee pickle\n",
      "Topic 7: donut counter ass baking remark store knife bout method lobby\n",
      "Topic 8: drive window employee car manager minute location drive-thru girl store\n",
      "Topic 9: chicken drive gravy minute meal girl transaction house attitude employee\n",
      "Topic 10: drink event garlic venue variety flavor cabbage fun night selection\n",
      "Topic 11: meat bark steak meal chicken sauce salad pork waiter sample\n",
      "Topic 12: attitude rice theme line pick kick local view music season\n",
      "Topic 13: shrimp crab sisig sauce fry seafood garlic buffet bang crawfish\n",
      "Topic 14: theme street plastic finger mint school offering search turkey tuna\n",
      "Topic 15: heating letdown rice fish sauce meal night foody seafood thai\n",
      "Topic 16: drink taco beef night chicken lunch rice sauce salsa meal\n",
      "Topic 17: contamination steak convention inconvenience restroom cross steakhouse meal realm lobster\n",
      "Topic 18: check street vinaigrette improvement woman sit moment batter range pleasure\n",
      "Topic 19: gps highway heel taste straw location fry correct number chicken\n",
      "Topic 20: latina rental cheese night drink salad wine breakfast celebration chicken\n",
      "Topic 21: border burrito chicken taco tripas buffet sauce bean rice meat\n",
      "Topic 22: location chicken taco fry burger breakfast cheese sauce salsa meal\n",
      "Topic 23: wing fry location ice straw collar highway number correct living\n",
      "Topic 24: shrimp sauce crab seafood sisig fry crawfish plastic bag business\n",
      "Topic 25: smh line dog twinky drumstick banana casino stand area item\n",
      "Topic 26: coffee bagel pastry breakfast gelato cafe chocolate crepe croissant morning\n",
      "Topic 27: state booth management child rating personality appetite issue venue compliment\n",
      "Topic 28: buffet selection station leg brunch line dessert section breakfast crab\n",
      "Topic 29: pat bar boxing bartender wing smoking home cook night production\n",
      "Topic 30: pizza salad sauce meal dinner cheese chicken flavor steak bread\n",
      "Topic 31: bark kalua coconut pig peppermint pineapple mango huli sample meat\n",
      "Topic 32: location subway sandwich decorum sanwich employee daughter mail coupon family\n",
      "Topic 33: pizza salad wine waiter sauce meal dinner crust flavor atmosphere\n",
      "Topic 34: pizza rice chicken lunch egg beef noodle sauce soup delivery\n",
      "Topic 35: bud piping patron ring yelp break smell bun cold perfection\n",
      "Topic 36: pupusa naan masala pupusas tikka paneer samosa plantain tandoori samosas\n",
      "Topic 37: cinnabon pizza cheese crust sauce slice buffet chicken salad breakfast\n",
      "Topic 38: sandwich crisps co-worker guy couple bread fry pool size burger\n",
      "Topic 39: chicken change salad sandwich nugget potato dressing today location bacon\n"
     ]
    }
   ],
   "source": [
    "topic_word = model_bis[0].T  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(lv_vocab10)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(u'Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf40 = NMF(n_components=40, init='random', random_state=0,max_iter=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf70 = NMF(n_components=70, init='random',random_state=0,max_iter=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat40 = nmf40.fit_transform(lv_dtm)\n",
    "mat70 = nmf70.fit_transform(lv_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3822, 40)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat40.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10364)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf40.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: sandwich bread sub subway location meat cheese lunch employee turkey\n",
      "Topic 1: cheese bean sauce pork chip flavor drink salsa beef tortilla\n",
      "Topic 2: rib sauce pork meat brisket potato flavor bbq tender bean\n",
      "Topic 3: cheese pancake location egg taste breakfast bacon flavor fry cake\n",
      "Topic 4: salad fry chicken location gyro meat sauce wrap meal juice\n",
      "Topic 5: soup pho pork rice beef roll egg broth noodle bowl\n",
      "Topic 6: meat beef friend meal item kind waitress quality selection area\n",
      "Topic 7: bar drink bartender beer night friend game area waitress atmosphere\n",
      "Topic 8: sushi roll fish sauce tuna rice shrimp chef tempura salmon\n",
      "Topic 9: chicken meal sauce drink salad bar location flavor cheese night\n",
      "Topic 10: pasta sauce bread cheese wine salad pizza meatball dinner tomato\n",
      "Topic 11: taco salsa burrito chip bean tortilla meat guacamole sauce enchilada\n",
      "Topic 12: flavor meal wine dessert beef sauce dinner steak meat duck\n",
      "Topic 13: sushi roll fish tuna dinner chef salad bar meal bread\n",
      "Topic 14: location meal minute waitress night manager drink waiter day strip\n",
      "Topic 15: dessert dinner wine chocolate meal room view cheese dining lobster\n",
      "Topic 16: wine night music waiter dinner friend sauce appetizer salad atmosphere\n",
      "Topic 17: chicken bar drink chip salsa bartender location wing lunch beer\n",
      "Topic 18: buffet selection lunch chicken dessert quality dinner item drink day\n",
      "Topic 19: bread salad meal sauce appetizer dinner wine flavor waiter pasta\n",
      "Topic 20: dinner drink night margarita waiter meal decor appetizer entree chicken\n",
      "Topic 21: lunch beef soup rice gyro owner salad chicken kabob special\n",
      "Topic 22: burger drink meal flavor item cheese taste friend tapas cocktail\n",
      "Topic 23: salad potato ice meal cream dessert flavor friend lunch coffee\n",
      "Topic 24: chicken salad sauce meal lunch location fry drink flavor cheese\n",
      "Topic 25: lunch sauce meal chicken dinner rice home day style shrimp\n",
      "Topic 26: pizza salad crust sauce cheese delivery garlic lunch pasta slice\n",
      "Topic 27: steak lobster dinner potato rib drink night waiter filet crab\n",
      "Topic 28: rice chicken bean meat bowl burrito meal flavor owner taste\n",
      "Topic 29: meal employee chicken night drink day location minute manager line\n",
      "Topic 30: sauce shrimp meal rice dinner location crab seafood garlic quality\n",
      "Topic 31: pizza slice cheese wing crust delivery minute day guy pepperoni\n",
      "Topic 32: shrimp chicken rice beef chow lunch mein seafood item dinner\n",
      "Topic 33: steak salad meal chip salsa soup dinner margarita waiter potato\n",
      "Topic 34: drive location window employee fry minute day ice car drink\n",
      "Topic 35: burger fry cheese sandwich onion bacon bun potato breakfast dog\n",
      "Topic 36: chicken sauce rice lunch egg beef flavor teriyaki roll delivery\n",
      "Topic 37: location taco burrito cheese dog employee line drive night burritos\n",
      "Topic 38: soup rice noodle thai chicken beef pad flavor lunch tea\n",
      "Topic 39: breakfast egg pancake coffee toast potato chicken waitress bacon hash\n"
     ]
    }
   ],
   "source": [
    "topic_word = nmf40.components_  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(lv_vocab10)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(u'Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: rice chicken soup sauce lunch roll noodle beef shrimp pork\n",
      "Topic 1: salsa bean sauce chip meat lunch rice burrito tortilla enchilada\n",
      "Topic 2: pho bar sandwich broth day beer bite night owner drink\n",
      "Topic 3: bar beer breakfast steak night lunch drink bartender home atmosphere\n",
      "Topic 4: burger cheese bar flavor fry salsa chip lunch drink beer\n",
      "Topic 5: meat sauce lunch burrito buffet flavor taco pork location salsa\n",
      "Topic 6: lunch tea sauce flavor owner gelato lamb item water noodle\n",
      "Topic 7: meal salad sandwich bread breakfast flavor rib taste sauce quality\n",
      "Topic 8: drink breakfast bar night friend cheese fry steak egg meal\n",
      "Topic 9: bar drink pizza bartender waitress beer burger night minute sandwich\n",
      "Topic 10: night sushi drink friend flavor breakfast roll pho egg meal\n",
      "Topic 11: drink beer shrimp night seafood bar store pasta day cheese\n",
      "Topic 12: taco breakfast egg steak sandwich meat location cheese toast line\n",
      "Topic 13: steak cheese meal dinner bread breakfast drink salad potato waiter\n",
      "Topic 14: breakfast buffet egg line coffee room hotel pancake location toast\n",
      "Topic 15: meal dinner shrimp seafood dessert night flavor buffet music drink\n",
      "Topic 16: chicken beef rice location shrimp lunch burger flavor minute noodle\n",
      "Topic 17: lobster pho noodle soup salad oyster pasta seafood bread dinner\n",
      "Topic 18: fry sandwich bagel burger fish drive employee manager court ice\n",
      "Topic 19: sauce drink bowl chicken taco burger location drive visit rice\n",
      "Topic 20: chicken night cheese drink pizza dog meal sauce minute bar\n",
      "Topic 21: fry cheese egg burger location breakfast taco chicken meal rice\n",
      "Topic 22: taco sauce meat steak cheese bean meal taste salsa family\n",
      "Topic 23: burger fry sushi potato onion rib breakfast cheese wine ring\n",
      "Topic 24: sushi fry roll burger lunch breakfast owner friend potato waitress\n",
      "Topic 25: bar drink meat sauce bartender owner bread flavor friend area\n",
      "Topic 26: sauce wine cheese flavor dinner dessert salad meal bread quality\n",
      "Topic 27: burger meal fry location steak salad waitress bread drink breakfast\n",
      "Topic 28: salsa location rice bean chip taco chicken strip tortilla stuff\n",
      "Topic 29: bar drink salad night sauce dinner beer bartender friend pasta\n",
      "Topic 30: bread subway location salad water pizza store sport hotel combo\n",
      "Topic 31: taco coffee cheese steak fry breakfast bar bean day chip\n",
      "Topic 32: location taco night drink day employee burrito drive bar burger\n",
      "Topic 33: area lunch ice location cream delivery day selection year store\n",
      "Topic 34: chicken salad meal lunch potato bar cheese burger bread breakfast\n",
      "Topic 35: burger fry salad employee gyro drink tea location drive day\n",
      "Topic 36: soup chicken rice noodle beef lunch sauce egg bar salad\n",
      "Topic 37: beef sandwich soup chicken coffee lunch breakfast day wing egg\n",
      "Topic 38: bar drink burger bartender steak beer salad potato game fry\n",
      "Topic 39: taco salsa drink sandwich chip breakfast bean fry bar margarita\n",
      "Topic 40: burger meal night waiter wine bread dinner sauce dessert friend\n",
      "Topic 41: salad bread dinner wine meal pasta sauce cheese waiter sushi\n",
      "Topic 42: breakfast cheese coffee egg waiter chocolate morning night location waitress\n",
      "Topic 43: pizza bar sandwich chip cheese drink day bartender salsa minute\n",
      "Topic 44: dinner bread meal waiter wine pizza steak sauce dessert bean\n",
      "Topic 45: chicken lunch rice sauce buffet pork meal shrimp location bean\n",
      "Topic 46: cheese salad taco court area bread flavor fry room sandwich\n",
      "Topic 47: taco location meal burger soup burrito fry sauce flavor salsa\n",
      "Topic 48: steak wine dinner night shrimp potato fish sauce wing egg\n",
      "Topic 49: sandwich salad steak sub lunch chip cheese bread location salsa\n",
      "Topic 50: sandwich location cheese burger sub fry kid employee line meat\n",
      "Topic 51: pizza breakfast egg crust slice cheese salad coffee minute day\n",
      "Topic 52: sushi roll delivery soup meat pho pizza gyro drive greek\n",
      "Topic 53: location drive drink bar burger sandwich window chicken employee fry\n",
      "Topic 54: chicken salad potato meal day steak buffet bar burger fry\n",
      "Topic 55: meat cheese coffee gyro sauce bean burrito taste flavor dog\n",
      "Topic 56: salad buffet sandwich chicken meat lunch flavor sauce pasta pork\n",
      "Topic 57: sushi roll bar fish chicken chef ice tuna selection quality\n",
      "Topic 58: sandwich bread buffet salad beef owner flavor day meat coffee\n",
      "Topic 59: sandwich breakfast coffee egg location pancake lunch waitress bread friend\n",
      "Topic 60: rice bean meat beef pork chicken lunch flavor salsa sauce\n",
      "Topic 61: dinner night beef waiter sauce bar roll meal sushi flavor\n",
      "Topic 62: night day pork bean chip beef salsa music friend owner\n",
      "Topic 63: taco fry location chicken salsa burrito bean cheese chip ice\n",
      "Topic 64: pizza salad cheese bread pasta meal sauce tomato lunch sandwich\n",
      "Topic 65: sauce fry dinner potato cake steak drink bar breakfast crab\n",
      "Topic 66: dog drive shrimp location fry minute kid employee window soda\n",
      "Topic 67: pizza sandwich sauce sub bread crust cheese lunch subway location\n",
      "Topic 68: chicken sandwich cheese pork soup sauce flavor beef lunch meat\n",
      "Topic 69: breakfast bar bartender drink location pancake guy day waitress lunch\n"
     ]
    }
   ],
   "source": [
    "topic_word = nmf70.components_  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(lv_vocab10)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(u'Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using KL divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kl_div_sym(v1,v2):\n",
    "    l = np.log(v1/v2)\n",
    "    return np.sum(v1*l) + np.sum(-v2*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = model_bis[1]\n",
    "kl_mat = np.zeros((3822,3822))\n",
    "for i in range(3822):\n",
    "    for j in range(i+1,3822,1):\n",
    "        kl_mat[i,j] = kl_mat[j,i] = kl_div_sym(G[i,:]/np.sum(G[i,:]),G[j,:]/np.sum(G[j,:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
