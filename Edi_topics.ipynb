{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import LDA\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import json\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('CS281/temp/Edi10.json', 'r') as fp:\n",
    "    reduced_edi = json.load(fp)\n",
    "\n",
    "with open('CS281/temp/word_to_index.json', 'r') as fp:\n",
    "    word_to_index = json.load(fp)\n",
    "\n",
    "with open('CS281/temp/index_to_word.json', 'r') as fp:\n",
    "    index_to_word = json.load(fp)\n",
    "\n",
    "with open('CS281/temp/bid_to_index.json', 'r') as fp:\n",
    "    bid_to_index = json.load(fp)\n",
    "\n",
    "with open('CS281/temp/Edi10.json', 'r') as fp:\n",
    "    index_to_bid = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab10 = word_to_index.keys()\n",
    "nonzero_data = []\n",
    "rows_s = []\n",
    "cols_s = []\n",
    "\n",
    "for k in reduced_edi.keys():\n",
    "    counter = collections.Counter(reduced_edi[k])\n",
    "    nonzero_data += counter.values()\n",
    "    rows_s += [bid_to_index[k]]*len(counter.values())\n",
    "    cols_s += [word_to_index[ck] for ck in counter.keys()]\n",
    "\n",
    "sparse_mat = sparse.csc_matrix((nonzero_data,(rows_s,cols_s)),shape = (len(bid_to_index),len(word_to_index)))\n",
    "\n",
    "dtm_edi = sparse_mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "7\n",
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "K = [5,7,10,15]\n",
    "ldas = {}\n",
    "perp = {}\n",
    "for k in K:\n",
    "    print k\n",
    "    np.random.seed(10000)\n",
    "    ldas[k] = LDA.lda_minibatch(dtm_edi[:580,:],k,10,512,0.6,100)\n",
    "    perp[k] = LDA.perplexity_test(ldas[k],dtm_edi[580:,],100)\n",
    "\n",
    "best = max(perp, key=perp.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 5\n",
      "Topic 0:\n",
      "\n",
      "[u'spoonful', u'icecream', u'punk', u'eating', u'chai', u'inconvenience', u'shoulder', u'pence', u'need', u'\\xa39.95']\n",
      "\n",
      "Topic 1:\n",
      "\n",
      "[u'nut', u'wood', u'cheeseburger', u'baker', u'vegetarian', u'road', u'feature', u'la', u'frontage', u'purchase']\n",
      "\n",
      "Topic 2:\n",
      "\n",
      "[u'research', u'pil', u'friend', u'ginger', u'crust', u'crisps', u'recipe', u'toast', u'university', u'eating']\n",
      "\n",
      "Topic 3:\n",
      "\n",
      "[u'broccoli', u'presence', u'round', u'calory', u'hazelnut', u'heap', u'general', u'branch', u'handful', u'noise']\n",
      "\n",
      "Topic 4:\n",
      "\n",
      "[u'whilst', u'walk', u'fall', u'slab', u'liver', u'hang', u'feature', u'chipotle', u'travel', u'kilo']\n",
      "\n",
      "Number of topics: 7\n",
      "***** BEST K *****\n",
      "Topic 0:\n",
      "\n",
      "[u'presence', u'round', u'calory', u'heap', u'branch', u'handful', u'noise', u'dauphinoise', u'parcel', u'creature']\n",
      "\n",
      "Topic 1:\n",
      "\n",
      "[u'punk', u'inconvenience', u'feature', u'shoulder', u'chipotle', u'date', u'relaxing', u'kilt', u'cousin', u'artwork']\n",
      "\n",
      "Topic 2:\n",
      "\n",
      "[u'broccoli', u'fall', u'hang', u'slab', u'liver', u'gap', u'gastropub', u'brim', u'waste', u'tex']\n",
      "\n",
      "Topic 3:\n",
      "\n",
      "[u'spoonful', u'icecream', u'eating', u'research', u'chai', u'pence', u'need', u'\\xa39.95', u'task', u'general']\n",
      "\n",
      "Topic 4:\n",
      "\n",
      "[u'nut', u'wood', u'cheeseburger', u'baker', u'vegetarian', u'road', u'la', u'porridge', u'frontage', u'feature']\n",
      "\n",
      "Topic 5:\n",
      "\n",
      "[u'whilst', u'walk', u'travel', u'kilo', u'companion', u'deliciousness', u'award', u'pork', u'warmth', u'mayo']\n",
      "\n",
      "Topic 6:\n",
      "\n",
      "[u'crust', u'spoonful', u'research', u'eating', u'broccoli', u'relaxing', u'shoulder', u'icecream', u'pence', u'\\xa39.95']\n",
      "\n",
      "Number of topics: 10\n",
      "Topic 0:\n",
      "\n",
      "[u'spoonful', u'icecream', u'eating', u'research', u'chai', u'pence', u'need', u'\\xa39.95', u'task', u'general']\n",
      "\n",
      "Topic 1:\n",
      "\n",
      "[u'crust', u'research', u'spoonful', u'eating', u'broccoli', u'shoulder', u'pence', u'relaxing', u'icecream', u'inconvenience']\n",
      "\n",
      "Topic 2:\n",
      "\n",
      "[u'whilst', u'walk', u'travel', u'kilo', u'companion', u'award', u'deliciousness', u'pork', u'warmth', u'mayo']\n",
      "\n",
      "Topic 3:\n",
      "\n",
      "[u'research', u'crust', u'spoonful', u'eating', u'relaxing', u'icecream', u'shoulder', u'broccoli', u'pence', u'nut']\n",
      "\n",
      "Topic 4:\n",
      "\n",
      "[u'punk', u'inconvenience', u'feature', u'shoulder', u'relaxing', u'chipotle', u'date', u'artwork', u'kilt', u'cousin']\n",
      "\n",
      "Topic 5:\n",
      "\n",
      "[u'courtesy', u'inch', u'medium', u'pre', u'plus', u'sat', u'hand', u'crust', u'focus', u'spoonful']\n",
      "\n",
      "Topic 6:\n",
      "\n",
      "[u'fall', u'hang', u'liver', u'gap', u'brim', u'waste', u'slab', u'selling', u'audience', u'tex']\n",
      "\n",
      "Topic 7:\n",
      "\n",
      "[u'broccoli', u'hazelnut', u'stand', u'pal', u'general', u'quote', u'rush', u'husband', u'brie', u'delicatessen']\n",
      "\n",
      "Topic 8:\n",
      "\n",
      "[u'crust', u'eating', u'spoonful', u'research', u'broccoli', u'icecream', u'pence', u'relaxing', u'shoulder', u'inconvenience']\n",
      "\n",
      "Topic 9:\n",
      "\n",
      "[u'gastropub', u'nut', u'wood', u'presence', u'cheeseburger', u'caf\\xe9s', u'baker', u'round', u'calory', u'vegetarian']\n",
      "\n",
      "Number of topics: 15\n",
      "Topic 0:\n",
      "\n",
      "[u'crust', u'research', u'spoonful', u'eating', u'broccoli', u'pence', u'shoulder', u'icecream', u'relaxing', u'design']\n",
      "\n",
      "Topic 1:\n",
      "\n",
      "[u'wine', u'tartan', u'couple', u'presence', u'round', u'light', u'chilli', u'caf\\xe9s', u'leek', u'frontage']\n",
      "\n",
      "Topic 2:\n",
      "\n",
      "[u'award', u'boot', u'mayo', u'oyster', u'tower', u'deliciousness', u'array', u'spoonful', u'crust', u'eating']\n",
      "\n",
      "Topic 3:\n",
      "\n",
      "[u'eating', u'crust', u'spoonful', u'research', u'relaxing', u'broccoli', u'icecream', u'shoulder', u'pence', u'nut']\n",
      "\n",
      "Topic 4:\n",
      "\n",
      "[u'crust', u'eating', u'spoonful', u'research', u'shoulder', u'icecream', u'relaxing', u'broccoli', u'pence', u'nut']\n",
      "\n",
      "Topic 5:\n",
      "\n",
      "[u'whilst', u'broccoli', u'walk', u'fall', u'slab', u'hang', u'liver', u'travel', u'kilo', u'design']\n",
      "\n",
      "Topic 6:\n",
      "\n",
      "[u'feature', u'nut', u'wood', u'cheeseburger', u'baker', u'gap', u'vegetarian', u'shellfish', u'road', u'twist']\n",
      "\n",
      "Topic 7:\n",
      "\n",
      "[u'crust', u'research', u'eating', u'spoonful', u'icecream', u'shoulder', u'broccoli', u'relaxing', u'pence', u'\\xa39.95']\n",
      "\n",
      "Topic 8:\n",
      "\n",
      "[u'crust', u'research', u'spoonful', u'eating', u'pence', u'shoulder', u'relaxing', u'broccoli', u'icecream', u'punk']\n",
      "\n",
      "Topic 9:\n",
      "\n",
      "[u'spoonful', u'crust', u'eating', u'research', u'icecream', u'pence', u'relaxing', u'shoulder', u'broccoli', u'inconvenience']\n",
      "\n",
      "Topic 10:\n",
      "\n",
      "[u'crust', u'research', u'spoonful', u'eating', u'broccoli', u'pence', u'relaxing', u'icecream', u'shoulder', u'travel']\n",
      "\n",
      "Topic 11:\n",
      "\n",
      "[u'crust', u'eating', u'spoonful', u'research', u'broccoli', u'icecream', u'shoulder', u'relaxing', u'pence', u'task']\n",
      "\n",
      "Topic 12:\n",
      "\n",
      "[u'research', u'pil', u'interior', u'university', u'friend', u'ginger', u'samosas', u'go-to', u'quirk', u'popularity']\n",
      "\n",
      "Topic 13:\n",
      "\n",
      "[u'spoonful', u'icecream', u'punk', u'eating', u'chai', u'inconvenience', u'relaxing', u'shoulder', u'pence', u'chipotle']\n",
      "\n",
      "Topic 14:\n",
      "\n",
      "[u'crust', u'research', u'eating', u'spoonful', u'shoulder', u'pence', u'broccoli', u'icecream', u'relaxing', u'inconvenience']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in K:\n",
    "    print \"Number of topics: %s\" % k\n",
    "    if k==best:\n",
    "        print \"***** BEST K *****\"\n",
    "    LDA.show_topics(ldas[k],range(k),10,vocab10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 0.6)\n",
      "(256, 0.7)\n",
      "(256, 0.8)\n",
      "(512, 0.6)\n",
      "(512, 0.7)\n",
      "(512, 0.8)\n",
      "(1024, 0.6)\n",
      "(1024, 0.7)\n",
      "(1024, 0.8)\n"
     ]
    }
   ],
   "source": [
    "perp2 = {}\n",
    "tau = [256,512,1024]\n",
    "kappa = [0.6,0.7,0.8]\n",
    "\n",
    "ldas2 = {}\n",
    "for t in tau:\n",
    "    for k in kappa:\n",
    "        print (t,k)\n",
    "        np.random.seed(10)\n",
    "        ldas2[(t,k)] = LDA.lda_minibatch(dtm_edi[:580,:],best,10,t,k,100)\n",
    "        perp2[(t,k)] = LDA.perplexity_test(ldas2[(t,k)],dtm_edi[580:,],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_combi = max(perp2, key=perp.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 0.8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BEST_EDI = LDA.lda_minibatch(dtm_edi,best,10,512,0.8,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 6)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_EDI[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"CS281/temp/edi_topics\",BEST_EDI[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"CS281/temp/edi_assignment\",BEST_EDI[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_topics(lda,thresh1):\n",
    "    topics = lda[0].copy()\n",
    "    gammas = lda[1].copy()\n",
    "    notflag_uni = []\n",
    "    for i in range(topics.shape[1]):\n",
    "        if 1 - spatial.distance.cosine(topics[:,i], [1./topics.shape[0]]*topics.shape[0])<thresh1:\n",
    "            notflag_uni.append(i)\n",
    "    newtopics = topics[:,notflag_uni]\n",
    "    newgammas = gammas[:,notflag_uni]/gammas[:,notflag_uni].sum(axis=1)[:,np.newaxis]\n",
    "    \n",
    "    \n",
    "    return newtopics,newgammas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
